{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "# Raw Data Loading\n",
    "train = pd.read_csv('./data/digit-recognizer/train.csv')\n",
    "test = pd.read_csv('./data/digit-recognizer/test.csv')\n",
    "\n",
    "# # Data Split\n",
    "# # 7:3 비율로 train과 test 분리\n",
    "# # x_data_test, t_data_test는 맨끝에서 모델의 최종 Accuracy를 측정할 때 딱 한번 사용\n",
    "# x_data_train, x_data_test, t_data_train, t_data_test = \\\n",
    "# train_test_split(df[['height','weight']], df['label'], test_size=0.3, random_state=0)\n",
    "\n",
    "# # Normalization\n",
    "# scaler = MinMaxScaler()\n",
    "# scaler.fit(x_data_train)  # scaling을 하기 위한 정보를 scaler에 setting\n",
    "\n",
    "# x_data_train_norm = scaler.transform(x_data_train)\n",
    "# x_data_test_norm = scaler.transform(x_data_test)\n",
    "\n",
    "# # Tensorflow 구현\n",
    "# # 해당 데이터는 Multinomial이기 때문에 One-Hot Encoding일 이용해 데이터 변환 필요\n",
    "# # 0 => 1 0 0\n",
    "# # 1 => 0 1 0\n",
    "# # 2 => 0 0 1\n",
    "# sess = tf.Session()\n",
    "# t_data_train_onehot = sess.run(tf.one_hot(t_data_train, depth=3))\n",
    "# t_data_test_onehot = sess.run(tf.one_hot(t_data_test, depth=3))\n",
    "\n",
    "# # Placeholder\n",
    "# X = tf.placeholder(shape=[None,2], dtype=tf.float32)\n",
    "# T = tf.placeholder(shape=[None,3], dtype=tf.float32)\n",
    "\n",
    "# # Weight & bias\n",
    "# W = tf.Variable(tf.random.normal([2,3]), name='weight')\n",
    "# b = tf.Variable(tf.random.normal([3]), name='bias')\n",
    "\n",
    "# # Hypothesis\n",
    "# logit = tf.matmul(X,W) + b\n",
    "# H = tf.nn.softmax(logit)\n",
    "\n",
    "# # Loss Fucntion\n",
    "# loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=T))\n",
    "\n",
    "# # Train\n",
    "# train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(loss)\n",
    "\n",
    "# # 반복학습\n",
    "# # parameter\n",
    "# num_of_epoch = 1000\n",
    "# batch_size = 100     # 한번에 학습할 x_data와 t_data의 행의 수\n",
    "\n",
    "# def run_train(sess, train_x, train_t):\n",
    "#     print('======= 학습시작 =======')\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     total_batch = int(train_x.shape[0] / batch_size)\n",
    "    \n",
    "#     for step in range(num_of_epoch):\n",
    "#         for i in range(total_batch):\n",
    "#             batch_x = train_x[i*batch_size:(i+1)*batch_size]\n",
    "#             batch_t = train_t[i*batch_size:(i+1)*batch_size]\n",
    "#             _, loss_val = sess.run([train,loss], feed_dict={X : batch_x, T : batch_t})\n",
    "\n",
    "#         if step % 100 == 0 :\n",
    "#             print('Loss : {}'.format(loss_val))\n",
    "#     print('======= 학습종료 =======')\n",
    "    \n",
    "# # Evaluation(Accuracy)\n",
    "# predict = tf.argmax(H,1)  # 1 => axis를 의미\n",
    "# correct = tf.equal(predict, tf.argmax(T,1))\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "\n",
    "# # Cross Validation\n",
    "# cv = 5        # KFold의 K\n",
    "# results = []   # 각 set에서 구한 accuracy를 집어넣을 list\n",
    "# kf = KFold(n_splits=cv, shuffle=True)\n",
    "\n",
    "# for training_idx, validation_idx in kf.split(x_data_train_norm):\n",
    "#     train_x = x_data_train_norm[training_idx]\n",
    "#     train_t = t_data_train_onehot[training_idx]\n",
    "#     valid_x = x_data_train_norm[validation_idx]\n",
    "#     valid_t = t_data_train_onehot[validation_idx]\n",
    "\n",
    "#     run_train(sess, train_x, train_t)\n",
    "#     results.append(sess.run(accuracy, feed_dict={X: valid_x, T : valid_t}))\n",
    "\n",
    "# print('Cross Validation 결과 : {}'.format(np.mean(results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Split\n",
    "x_data_train, x_data_test, t_data_train, t_data_test = \\\n",
    "train_test_split(train.iloc[:,1:], train['label'], test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Normalization\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x_data_train)  # scaling을 하기 위한 정보를 scaler에 setting\n",
    "\n",
    "x_data_train_norm = scaler.transform(x_data_train)\n",
    "x_data_test_norm = scaler.transform(x_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "t_data_train_onehot = sess.run(tf.one_hot(t_data_train, depth=10))\n",
    "t_data_test_onehot = sess.run(tf.one_hot(t_data_test, depth=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder\n",
    "X = tf.placeholder(shape=[None,784], dtype=tf.float32)\n",
    "T = tf.placeholder(shape=[None,10], dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "W = tf.Variable(tf.random.normal([784,10]), name='weight')\n",
    "b = tf.Variable(tf.random.normal([10]), name='bias')\n",
    "\n",
    "# Hypothesis\n",
    "logit = tf.matmul(X,W) + b\n",
    "H = tf.nn.softmax(logit)\n",
    "\n",
    "# Loss Fucntion\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=T))\n",
    "\n",
    "# Train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(loss)\n",
    "\n",
    "# 반복학습\n",
    "# parameter\n",
    "num_of_epoch = 1000\n",
    "batch_size = 100     # 한번에 학습할 x_data와 t_data의 행의 수\n",
    "\n",
    "def run_train(sess, train_x, train_t):\n",
    "    print('======= 학습시작 =======')\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    total_batch = int(train_x.shape[0] / batch_size)\n",
    "    \n",
    "    for step in range(num_of_epoch):\n",
    "        for i in range(total_batch):\n",
    "            batch_x = train_x[i*batch_size:(i+1)*batch_size]\n",
    "            batch_t = train_t[i*batch_size:(i+1)*batch_size]\n",
    "            _, loss_val = sess.run([train,loss], feed_dict={X : batch_x, T : batch_t})\n",
    "\n",
    "        if step % 100 == 0 :\n",
    "            print('Loss : {}'.format(loss_val))\n",
    "    print('======= 학습종료 =======')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation(Accuracy)\n",
    "predict = tf.argmax(H,1)  # 1 => axis를 의미\n",
    "correct = tf.equal(predict, tf.argmax(T,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "\n",
    "# Cross Validation\n",
    "cv = 5        # KFold의 K\n",
    "results = []   # 각 set에서 구한 accuracy를 집어넣을 list\n",
    "kf = KFold(n_splits=cv, shuffle=True)\n",
    "\n",
    "for training_idx, validation_idx in kf.split(x_data_train_norm):\n",
    "    train_x = x_data_train_norm[training_idx]\n",
    "    train_t = t_data_train_onehot[training_idx]\n",
    "    valid_x = x_data_train_norm[validation_idx]\n",
    "    valid_t = t_data_train_onehot[validation_idx]\n",
    "\n",
    "    run_train(sess, train_x, train_t)\n",
    "    results.append(sess.run(accuracy, feed_dict={X: valid_x, T : valid_t}))\n",
    "\n",
    "print('Cross Validation 결과 : {}'.format(np.mean(results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습진행\n",
    "run_train(sess, x_data_train_norm, t_data_train_onehot)\n",
    "\n",
    "# Accuracy 측정\n",
    "result = sess.run(accuracy, feed_dict={X: x_data_test_norm, T : t_data_test_onehot})\n",
    "print('최종 Accuracy : {}'.format(result))\n",
    "\n",
    "# Prediction\n",
    "test_scaled = scaler.transform(test)\n",
    "\n",
    "result = sess.run(H, feed_dict={X : test_scaled})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.argmax(result[0]))\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "imageid = list(range(1,result.shape[0]+1))\n",
    "label = []\n",
    "\n",
    "for i in range(result.shape[0]):\n",
    "    label.append(np.argmax(result[i]))\n",
    "    \n",
    "df['ImageId'] = imageid\n",
    "df['Label'] = label\n",
    "\n",
    "df.to_csv('./submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2020/10/16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset\n",
    "\n",
    "# Tensorflow 1.15버전\n",
    "# multinomial classification으로 MNIST 구현\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler        # Normalization\n",
    "from sklearn.model_selection import train_test_split  # train, test 분리\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Raw Data Loading\n",
    "df = pd.read_csv('./data/mnist/train.csv')\n",
    "\n",
    "##### 결측치와 이상치는 없음 #####\n",
    "\n",
    "##### 이미지 확인 #####\n",
    "img_data = df.drop('label', axis=1, inplace=False).values\n",
    "\n",
    "fig = plt.figure()\n",
    "fig_arr = []\n",
    "\n",
    "for n in range(10):\n",
    "    fig_arr.append(fig.add_subplot(2,5,n+1))\n",
    "    fig_arr[n].imshow(img_data[n].reshape(28,28), cmap='Greys')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Data Split\n",
    "x_data_train, x_data_test, t_data_train, t_data_test = \\\n",
    "train_test_split(df.drop('label', axis=1, inplace=False), df['label'], test_size=0.3, random_state=0)\n",
    "\n",
    "# Min-Max Normalization\n",
    "scaler = MinMaxScaler()   # scaler = StandardScaler()\n",
    "scaler.fit(x_data_train)\n",
    "x_data_train_norm = scaler.transform(x_data_train)\n",
    "x_data_test_norm = scaler.transform(x_data_test)\n",
    "\n",
    "del x_data_train\n",
    "del x_data_test\n",
    "\n",
    "##### Tensorflow implementation #####\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "t_data_train_onehot = sess.run(tf.one_hot(t_data_train,depth=10))\n",
    "t_data_test_onehot = sess.run(tf.one_hot(t_data_test,depth=10))\n",
    "\n",
    "# Placeholder\n",
    "X = tf.placeholder(shape=[None,784], dtype=tf.float32)\n",
    "T = tf.placeholder(shape=[None,10], dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "W = tf.Variable(tf.random.normal([784,10]), name='weight')\n",
    "b = tf.Variable(tf.random.normal([10]), name='bias')\n",
    "\n",
    "# Hypothesis\n",
    "logit = tf.matmul(X,W) + b\n",
    "H = tf.nn.softmax(logit)   # softmax activation function\n",
    "\n",
    "# loss\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, \n",
    "                                                                 labels=T))\n",
    "\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-1).minimize(loss)\n",
    "\n",
    "# parameter\n",
    "num_of_epoch = 200\n",
    "batch_size = 100\n",
    "\n",
    "\n",
    "# 학습\n",
    "def run_train(sess, train_x, train_t):\n",
    "    print('### Starting Training ###')\n",
    "    # 초기화\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(num_of_epoch):\n",
    "        total_batch = int(train_x.shape[0] / batch_size)\n",
    "\n",
    "        for i in range(total_batch):\n",
    "            batch_x = train_x[i*batch_size:(i+1)*batch_size]\n",
    "            batch_t = train_t[i*batch_size:(i+1)*batch_size]\n",
    "            _, loss_val = sess.run([train,loss], feed_dict={X:batch_x, T:batch_t})\n",
    "\n",
    "        if step % 20 == 0:\n",
    "            print('Loss : {}'.format(loss_val))\n",
    "    print('### End Training ###')\n",
    "\n",
    "    \n",
    "# Accuracy\n",
    "predict = tf.argmax(H,1)\n",
    "\n",
    "   \n",
    "# Testing\n",
    "run_train(sess,x_data_train_norm,t_data_train_onehot) # 학습\n",
    "print('### Test Set으로 Accuracy 측정 ###')\n",
    "result = sess.run(predict, feed_dict={X:x_data_test_norm})\n",
    "print(classification_report(t_data_test,result.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset\n",
    "\n",
    "# Tensorflow 1.15버전\n",
    "# DNN(초기화, activation function, dropout처리 안함)으로 MNIST 구현\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler        # Normalization\n",
    "from sklearn.model_selection import train_test_split  # train, test 분리\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Raw Data Loading\n",
    "df = pd.read_csv('./data/digit-recognizer/train.csv')\n",
    "display(df.head(), df.shape)\n",
    "\n",
    "##### 결측치와 이상치는 없음 #####\n",
    "\n",
    "# Data Split\n",
    "x_data_train, x_data_test, t_data_train, t_data_test = \\\n",
    "train_test_split(df.drop('label', axis=1, inplace=False), df['label'], test_size=0.3, random_state=0)\n",
    "# test_size : test set의 비율 (0.3 => 30%)\n",
    "# random_state : split할 때 랜덤하게 split하게 되는데 이를 일정하게 고정(seed의 개념)\n",
    "\n",
    "# Min-Max Normalization\n",
    "scaler = MinMaxScaler()   # scaler = StandardScaler()\n",
    "scaler.fit(x_data_train)\n",
    "x_data_train_norm = scaler.transform(x_data_train)\n",
    "x_data_test_norm = scaler.transform(x_data_test)\n",
    "\n",
    "del x_data_train\n",
    "del x_data_test\n",
    "\n",
    "##### Tensorflow implementation #####\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "t_data_train_onehot = sess.run(tf.one_hot(t_data_train,depth=10))\n",
    "t_data_test_onehot = sess.run(tf.one_hot(t_data_test,depth=10))\n",
    "\n",
    "# del t_data_train\n",
    "# del t_data_test\n",
    "\n",
    "# Placeholder\n",
    "X = tf.placeholder(shape=[None,784], dtype=tf.float32)\n",
    "T = tf.placeholder(shape=[None,10], dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "W2 = tf.Variable(tf.random.normal([784,256]), name='weight2')\n",
    "b2 = tf.Variable(tf.random.normal([256]), name='bias2')\n",
    "layer2 = tf.sigmoid(tf.matmul(X,W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random.normal([256,128]), name='weight3')\n",
    "b3 = tf.Variable(tf.random.normal([128]), name='bias3')\n",
    "layer3 = tf.sigmoid(tf.matmul(layer2,W3) + b3)\n",
    "\n",
    "W4 = tf.Variable(tf.random.normal([128,10]), name='weight4')\n",
    "b4 = tf.Variable(tf.random.normal([10]), name='bias4')\n",
    "\n",
    "# Hypothesis\n",
    "logit = tf.matmul(layer3,W4) + b4\n",
    "H = tf.nn.softmax(logit)   # softmax activation function\n",
    "\n",
    "# loss\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, \n",
    "                                                                 labels=T))\n",
    "\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-1).minimize(loss)\n",
    "\n",
    "# parameter\n",
    "num_of_epoch = 500\n",
    "batch_size = 100\n",
    "\n",
    "\n",
    "# 학습\n",
    "def run_train(sess, train_x, train_t):\n",
    "    print('### Starting Training ###')\n",
    "    # 초기화\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(num_of_epoch):\n",
    "        total_batch = int(train_x.shape[0] / batch_size)\n",
    "\n",
    "        for i in range(total_batch):\n",
    "            batch_x = train_x[i*batch_size:(i+1)*batch_size]\n",
    "            batch_t = train_t[i*batch_size:(i+1)*batch_size]\n",
    "            _, loss_val = sess.run([train,loss], feed_dict={X:batch_x, T:batch_t})\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            print('Loss : {}'.format(loss_val))\n",
    "    print('### End Training ###')\n",
    "\n",
    "    \n",
    "# Accuracy\n",
    "predict = tf.argmax(H,1)\n",
    "correct = tf.equal(predict, tf.argmax(T,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "\n",
    "# Testing\n",
    "run_train(sess,x_data_train_norm,t_data_train_onehot) # 학습\n",
    "print('### Test Set으로 Accuracy 측정 ###')\n",
    "result = sess.run(predict, feed_dict={X:x_data_test_norm})\n",
    "print(classification_report(t_data_test,result.ravel()))\n",
    "# Test Set 정확도 : 0.93"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset\n",
    "\n",
    "# Tensorflow 1.15버전\n",
    "# DNN(dropout처리 안함)으로 MNIST 구현\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler        # Normalization\n",
    "from sklearn.model_selection import train_test_split  # train, test 분리\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Raw Data Loading\n",
    "df = pd.read_csv('./data/digit-recognizer/train.csv')\n",
    "display(df.head(), df.shape)\n",
    "\n",
    "##### 결측치와 이상치는 없음 #####\n",
    "\n",
    "# Data Split\n",
    "x_data_train, x_data_test, t_data_train, t_data_test = \\\n",
    "train_test_split(df.drop('label', axis=1, inplace=False), df['label'], test_size=0.3, random_state=0)\n",
    "# test_size : test set의 비율 (0.3 => 30%)\n",
    "# random_state : split할 때 랜덤하게 split하게 되는데 이를 일정하게 고정(seed의 개념)\n",
    "\n",
    "# Min-Max Normalization\n",
    "scaler = MinMaxScaler()   # scaler = StandardScaler()\n",
    "scaler.fit(x_data_train)\n",
    "x_data_train_norm = scaler.transform(x_data_train)\n",
    "x_data_test_norm = scaler.transform(x_data_test)\n",
    "\n",
    "del x_data_train\n",
    "del x_data_test\n",
    "\n",
    "##### Tensorflow implementation #####\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "t_data_train_onehot = sess.run(tf.one_hot(t_data_train,depth=10))\n",
    "t_data_test_onehot = sess.run(tf.one_hot(t_data_test,depth=10))\n",
    "\n",
    "# del t_data_train\n",
    "# del t_data_test\n",
    "\n",
    "# Placeholder\n",
    "X = tf.placeholder(shape=[None,784], dtype=tf.float32)\n",
    "T = tf.placeholder(shape=[None,10], dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "# He's 초기화\n",
    "W2 = tf.get_variable('w2', shape=[784,256], initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "b2 = tf.Variable(tf.random.normal([256]), name='bias2')\n",
    "layer2 = tf.nn.relu(tf.matmul(X,W2) + b2)\n",
    "\n",
    "W3 = tf.get_variable('w3', shape=[256,128], initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "b3 = tf.Variable(tf.random.normal([128]), name='bias3')\n",
    "layer3 = tf.nn.relu(tf.matmul(layer2,W3) + b3)\n",
    "\n",
    "W4 = tf.get_variable('w4', shape=[128,10], initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "b4 = tf.Variable(tf.random.normal([10]), name='bias4')\n",
    "\n",
    "# Hypothesis\n",
    "logit = tf.matmul(layer3,W4) + b4\n",
    "H = tf.nn.softmax(logit)   # softmax activation function\n",
    "\n",
    "# loss\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, \n",
    "                                                                 labels=T))\n",
    "\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-1).minimize(loss)\n",
    "\n",
    "# parameter\n",
    "num_of_epoch = 500\n",
    "batch_size = 100\n",
    "\n",
    "\n",
    "# 학습\n",
    "def run_train(sess, train_x, train_t):\n",
    "    print('### Starting Training ###')\n",
    "    # 초기화\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(num_of_epoch):\n",
    "        total_batch = int(train_x.shape[0] / batch_size)\n",
    "\n",
    "        for i in range(total_batch):\n",
    "            batch_x = train_x[i*batch_size:(i+1)*batch_size]\n",
    "            batch_t = train_t[i*batch_size:(i+1)*batch_size]\n",
    "            _, loss_val = sess.run([train,loss], feed_dict={X:batch_x, T:batch_t})\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            print('Loss : {}'.format(loss_val))\n",
    "    print('### End Training ###')\n",
    "\n",
    "    \n",
    "# Accuracy\n",
    "predict = tf.argmax(H,1)\n",
    "correct = tf.equal(predict, tf.argmax(T,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "\n",
    "# Testing\n",
    "run_train(sess,x_data_train_norm,t_data_train_onehot) # 학습\n",
    "print('### Test Set으로 Accuracy 측정 ###')\n",
    "result = sess.run(predict, feed_dict={X:x_data_test_norm})\n",
    "print(classification_report(t_data_test,result.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(42000, 785)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Starting Training ###\n",
      "Loss : 0.3780944049358368\n",
      "Loss : 0.040455326437950134\n",
      "Loss : 0.0035565667785704136\n",
      "Loss : 0.0036169844679534435\n",
      "Loss : 0.005395825952291489\n",
      "Loss : 0.004061732906848192\n",
      "Loss : 0.0022698023822158575\n",
      "Loss : 0.007384584750980139\n",
      "Loss : 0.0005915524670854211\n",
      "Loss : 0.00011184781033080071\n",
      "### End Training ###\n",
      "### Test Set으로 Accuracy 측정 ###\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98      1242\n",
      "           1       0.98      0.99      0.99      1429\n",
      "           2       0.97      0.99      0.98      1276\n",
      "           3       0.98      0.96      0.97      1298\n",
      "           4       0.97      0.98      0.98      1236\n",
      "           5       0.97      0.98      0.98      1119\n",
      "           6       0.98      0.99      0.99      1243\n",
      "           7       0.98      0.98      0.98      1334\n",
      "           8       0.97      0.96      0.97      1204\n",
      "           9       0.98      0.95      0.97      1219\n",
      "\n",
      "    accuracy                           0.98     12600\n",
      "   macro avg       0.98      0.98      0.98     12600\n",
      "weighted avg       0.98      0.98      0.98     12600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tensorflow 1.15버전\n",
    "# DNN으로 MNIST 구현\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler        # Normalization\n",
    "from sklearn.model_selection import train_test_split  # train, test 분리\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 그래프 초기화\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Raw Data Loading\n",
    "df = pd.read_csv('./data/digit-recognizer/train.csv')\n",
    "display(df.head(), df.shape)\n",
    "\n",
    "##### 결측치와 이상치는 없음 #####\n",
    "\n",
    "# Data Split\n",
    "x_data_train, x_data_test, t_data_train, t_data_test = \\\n",
    "train_test_split(df.drop('label', axis=1, inplace=False), df['label'], test_size=0.3, random_state=0)\n",
    "# test_size : test set의 비율 (0.3 => 30%)\n",
    "# random_state : split할 때 랜덤하게 split하게 되는데 이를 일정하게 고정(seed의 개념)\n",
    "\n",
    "# Min-Max Normalization\n",
    "scaler = MinMaxScaler()   # scaler = StandardScaler()\n",
    "scaler.fit(x_data_train)\n",
    "x_data_train_norm = scaler.transform(x_data_train)\n",
    "x_data_test_norm = scaler.transform(x_data_test)\n",
    "\n",
    "del x_data_train\n",
    "del x_data_test\n",
    "\n",
    "##### Tensorflow implementation #####\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "t_data_train_onehot = sess.run(tf.one_hot(t_data_train,depth=10))\n",
    "t_data_test_onehot = sess.run(tf.one_hot(t_data_test,depth=10))\n",
    "\n",
    "# del t_data_train\n",
    "# del t_data_test\n",
    "\n",
    "# Placeholder\n",
    "X = tf.placeholder(shape=[None,784], dtype=tf.float32)\n",
    "T = tf.placeholder(shape=[None,10], dtype=tf.float32)\n",
    "drop_rate = tf.placeholder(dtype=tf.float32)  # dropout할 떄 사용할 비율\n",
    "\n",
    "# Weight & bias\n",
    "# He's 초기화\n",
    "W1 = tf.get_variable('w1', shape=[784,256], initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "b1 = tf.Variable(tf.random.normal([256]), name='bias1')\n",
    "_hidden1 = tf.nn.relu(tf.matmul(X,W1) + b1)\n",
    "hidden1 = tf.nn.dropout(_hidden1, rate=drop_rate)\n",
    "\n",
    "W2 = tf.get_variable('w2', shape=[256,128], initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "b2 = tf.Variable(tf.random.normal([128]), name='bias2')\n",
    "_hidden2 = tf.nn.relu(tf.matmul(hidden1,W2) + b2)\n",
    "hidden2 = tf.nn.dropout(_hidden2, rate=drop_rate)\n",
    "\n",
    "W3 = tf.get_variable('w3', shape=[128,10], initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "b3 = tf.Variable(tf.random.normal([10]), name='bias3')\n",
    "\n",
    "# Hypothesis\n",
    "logit = tf.matmul(hidden2,W3) + b3\n",
    "H = tf.nn.softmax(logit)   # softmax activation function\n",
    "\n",
    "# loss\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, \n",
    "                                                                 labels=T))\n",
    "\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-1).minimize(loss)\n",
    "\n",
    "# parameter\n",
    "num_of_epoch = 500\n",
    "batch_size = 100\n",
    "\n",
    "\n",
    "# 학습\n",
    "def run_train(sess, train_x, train_t):\n",
    "    print('### Starting Training ###')\n",
    "    # 초기화\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(num_of_epoch):\n",
    "        total_batch = int(train_x.shape[0] / batch_size)\n",
    "\n",
    "        for i in range(total_batch):\n",
    "            batch_x = train_x[i*batch_size:(i+1)*batch_size]\n",
    "            batch_t = train_t[i*batch_size:(i+1)*batch_size]\n",
    "            _, loss_val = sess.run([train,loss], feed_dict={X:batch_x, T:batch_t, drop_rate:0.3})\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            print('Loss : {}'.format(loss_val))\n",
    "    print('### End Training ###')\n",
    "\n",
    "    \n",
    "# Accuracy\n",
    "predict = tf.argmax(H,1)\n",
    "correct = tf.equal(predict, tf.argmax(T,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "\n",
    "# Testing\n",
    "run_train(sess,x_data_train_norm,t_data_train_onehot) # 학습\n",
    "print('### Test Set으로 Accuracy 측정 ###')\n",
    "result = sess.run(predict, feed_dict={X:x_data_test_norm, drop_rate:0})  # test할 때는 모든 노드를 켠 상태에서 해야 함\n",
    "print(classification_report(t_data_test,result.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20580 samples, validate on 8820 samples\n",
      "Epoch 1/100\n",
      "20580/20580 [==============================] - 3s 159us/sample - loss: 0.5916 - sparse_categorical_accuracy: 0.8138 - val_loss: 0.2393 - val_sparse_categorical_accuracy: 0.9289\n",
      "Epoch 2/100\n",
      "20580/20580 [==============================] - 2s 96us/sample - loss: 0.2530 - sparse_categorical_accuracy: 0.9236 - val_loss: 0.1819 - val_sparse_categorical_accuracy: 0.9466\n",
      "Epoch 3/100\n",
      "20580/20580 [==============================] - 2s 97us/sample - loss: 0.1817 - sparse_categorical_accuracy: 0.9453 - val_loss: 0.1515 - val_sparse_categorical_accuracy: 0.9541\n",
      "Epoch 4/100\n",
      "20580/20580 [==============================] - 2s 95us/sample - loss: 0.1449 - sparse_categorical_accuracy: 0.9559 - val_loss: 0.1366 - val_sparse_categorical_accuracy: 0.9587\n",
      "Epoch 5/100\n",
      "20580/20580 [==============================] - 2s 88us/sample - loss: 0.1219 - sparse_categorical_accuracy: 0.9626 - val_loss: 0.1332 - val_sparse_categorical_accuracy: 0.9599\n",
      "Epoch 6/100\n",
      "20580/20580 [==============================] - 2s 97us/sample - loss: 0.1034 - sparse_categorical_accuracy: 0.9679 - val_loss: 0.1277 - val_sparse_categorical_accuracy: 0.9612\n",
      "Epoch 7/100\n",
      "20580/20580 [==============================] - 2s 102us/sample - loss: 0.0899 - sparse_categorical_accuracy: 0.9722 - val_loss: 0.1134 - val_sparse_categorical_accuracy: 0.9670\n",
      "Epoch 8/100\n",
      "20580/20580 [==============================] - 2s 101us/sample - loss: 0.0798 - sparse_categorical_accuracy: 0.9748 - val_loss: 0.1156 - val_sparse_categorical_accuracy: 0.9651\n",
      "Epoch 9/100\n",
      "20580/20580 [==============================] - 2s 90us/sample - loss: 0.0662 - sparse_categorical_accuracy: 0.9792 - val_loss: 0.1078 - val_sparse_categorical_accuracy: 0.9702\n",
      "Epoch 10/100\n",
      "20580/20580 [==============================] - 2s 94us/sample - loss: 0.0618 - sparse_categorical_accuracy: 0.9797 - val_loss: 0.1127 - val_sparse_categorical_accuracy: 0.9680\n",
      "Epoch 11/100\n",
      "20580/20580 [==============================] - 2s 89us/sample - loss: 0.0555 - sparse_categorical_accuracy: 0.9827 - val_loss: 0.1113 - val_sparse_categorical_accuracy: 0.9717\n",
      "Epoch 12/100\n",
      "20580/20580 [==============================] - 2s 92us/sample - loss: 0.0510 - sparse_categorical_accuracy: 0.9837 - val_loss: 0.1241 - val_sparse_categorical_accuracy: 0.9666\n",
      "Epoch 13/100\n",
      "20580/20580 [==============================] - 2s 89us/sample - loss: 0.0493 - sparse_categorical_accuracy: 0.9839 - val_loss: 0.1121 - val_sparse_categorical_accuracy: 0.9697\n",
      "Epoch 14/100\n",
      "20580/20580 [==============================] - 2s 91us/sample - loss: 0.0405 - sparse_categorical_accuracy: 0.9873 - val_loss: 0.1198 - val_sparse_categorical_accuracy: 0.9686\n",
      "Epoch 15/100\n",
      "20580/20580 [==============================] - 2s 92us/sample - loss: 0.0395 - sparse_categorical_accuracy: 0.9868 - val_loss: 0.1182 - val_sparse_categorical_accuracy: 0.9704\n",
      "Epoch 16/100\n",
      "20580/20580 [==============================] - 2s 100us/sample - loss: 0.0319 - sparse_categorical_accuracy: 0.9901 - val_loss: 0.1267 - val_sparse_categorical_accuracy: 0.9697\n",
      "Epoch 17/100\n",
      "20580/20580 [==============================] - 2s 104us/sample - loss: 0.0359 - sparse_categorical_accuracy: 0.9878 - val_loss: 0.1195 - val_sparse_categorical_accuracy: 0.9709\n",
      "Epoch 18/100\n",
      "20580/20580 [==============================] - 2s 98us/sample - loss: 0.0317 - sparse_categorical_accuracy: 0.9890 - val_loss: 0.1277 - val_sparse_categorical_accuracy: 0.9701\n",
      "Epoch 19/100\n",
      "20580/20580 [==============================] - 2s 101us/sample - loss: 0.0331 - sparse_categorical_accuracy: 0.9887 - val_loss: 0.1332 - val_sparse_categorical_accuracy: 0.9686\n",
      "Epoch 20/100\n",
      "20580/20580 [==============================] - 2s 102us/sample - loss: 0.0312 - sparse_categorical_accuracy: 0.9899 - val_loss: 0.1239 - val_sparse_categorical_accuracy: 0.9731\n",
      "Epoch 21/100\n",
      "20580/20580 [==============================] - 2s 94us/sample - loss: 0.0281 - sparse_categorical_accuracy: 0.9906 - val_loss: 0.1312 - val_sparse_categorical_accuracy: 0.9701\n",
      "Epoch 22/100\n",
      "20580/20580 [==============================] - 2s 99us/sample - loss: 0.0245 - sparse_categorical_accuracy: 0.9919 - val_loss: 0.1331 - val_sparse_categorical_accuracy: 0.9715\n",
      "Epoch 23/100\n",
      "20580/20580 [==============================] - 2s 93us/sample - loss: 0.0266 - sparse_categorical_accuracy: 0.9917 - val_loss: 0.1355 - val_sparse_categorical_accuracy: 0.9697\n",
      "Epoch 24/100\n",
      "20580/20580 [==============================] - 2s 94us/sample - loss: 0.0248 - sparse_categorical_accuracy: 0.9916 - val_loss: 0.1243 - val_sparse_categorical_accuracy: 0.9732\n",
      "Epoch 25/100\n",
      "20580/20580 [==============================] - 2s 109us/sample - loss: 0.0241 - sparse_categorical_accuracy: 0.9919 - val_loss: 0.1284 - val_sparse_categorical_accuracy: 0.9717\n",
      "Epoch 26/100\n",
      "20580/20580 [==============================] - 2s 92us/sample - loss: 0.0213 - sparse_categorical_accuracy: 0.9927 - val_loss: 0.1258 - val_sparse_categorical_accuracy: 0.9729\n",
      "Epoch 27/100\n",
      "20580/20580 [==============================] - 2s 91us/sample - loss: 0.0254 - sparse_categorical_accuracy: 0.9906 - val_loss: 0.1260 - val_sparse_categorical_accuracy: 0.9723\n",
      "Epoch 28/100\n",
      "20580/20580 [==============================] - 2s 103us/sample - loss: 0.0227 - sparse_categorical_accuracy: 0.9927 - val_loss: 0.1488 - val_sparse_categorical_accuracy: 0.9692\n",
      "Epoch 29/100\n",
      "20580/20580 [==============================] - 2s 92us/sample - loss: 0.0222 - sparse_categorical_accuracy: 0.9927 - val_loss: 0.1510 - val_sparse_categorical_accuracy: 0.9709\n",
      "Epoch 30/100\n",
      "20580/20580 [==============================] - 2s 96us/sample - loss: 0.0216 - sparse_categorical_accuracy: 0.9928 - val_loss: 0.1399 - val_sparse_categorical_accuracy: 0.9715\n",
      "Epoch 31/100\n",
      "20580/20580 [==============================] - 2s 93us/sample - loss: 0.0215 - sparse_categorical_accuracy: 0.9926 - val_loss: 0.1386 - val_sparse_categorical_accuracy: 0.9747\n",
      "Epoch 32/100\n",
      "20580/20580 [==============================] - 2s 92us/sample - loss: 0.0213 - sparse_categorical_accuracy: 0.9927 - val_loss: 0.1312 - val_sparse_categorical_accuracy: 0.9741\n",
      "Epoch 33/100\n",
      "20580/20580 [==============================] - 2s 88us/sample - loss: 0.0211 - sparse_categorical_accuracy: 0.9930 - val_loss: 0.1350 - val_sparse_categorical_accuracy: 0.9726\n",
      "Epoch 34/100\n",
      "20580/20580 [==============================] - 2s 89us/sample - loss: 0.0195 - sparse_categorical_accuracy: 0.9939 - val_loss: 0.1424 - val_sparse_categorical_accuracy: 0.9734\n",
      "Epoch 35/100\n",
      "20580/20580 [==============================] - 2s 89us/sample - loss: 0.0173 - sparse_categorical_accuracy: 0.9944 - val_loss: 0.1478 - val_sparse_categorical_accuracy: 0.9715\n",
      "Epoch 36/100\n",
      "20580/20580 [==============================] - 2s 90us/sample - loss: 0.0163 - sparse_categorical_accuracy: 0.9945 - val_loss: 0.1421 - val_sparse_categorical_accuracy: 0.9731\n",
      "Epoch 37/100\n",
      "20580/20580 [==============================] - 2s 86us/sample - loss: 0.0176 - sparse_categorical_accuracy: 0.9937 - val_loss: 0.1491 - val_sparse_categorical_accuracy: 0.9721\n",
      "Epoch 38/100\n",
      "20580/20580 [==============================] - 2s 89us/sample - loss: 0.0181 - sparse_categorical_accuracy: 0.9935 - val_loss: 0.1428 - val_sparse_categorical_accuracy: 0.9736\n",
      "Epoch 39/100\n",
      "20580/20580 [==============================] - 2s 87us/sample - loss: 0.0151 - sparse_categorical_accuracy: 0.9957 - val_loss: 0.1482 - val_sparse_categorical_accuracy: 0.9747\n",
      "Epoch 40/100\n",
      "20580/20580 [==============================] - 2s 89us/sample - loss: 0.0154 - sparse_categorical_accuracy: 0.9950 - val_loss: 0.1555 - val_sparse_categorical_accuracy: 0.9728\n",
      "Epoch 41/100\n",
      "20580/20580 [==============================] - 2s 88us/sample - loss: 0.0167 - sparse_categorical_accuracy: 0.9946 - val_loss: 0.1513 - val_sparse_categorical_accuracy: 0.9718\n",
      "Epoch 42/100\n",
      "20580/20580 [==============================] - 2s 90us/sample - loss: 0.0192 - sparse_categorical_accuracy: 0.9942 - val_loss: 0.1498 - val_sparse_categorical_accuracy: 0.9736\n",
      "Epoch 43/100\n",
      "20580/20580 [==============================] - 2s 87us/sample - loss: 0.0205 - sparse_categorical_accuracy: 0.9934 - val_loss: 0.1445 - val_sparse_categorical_accuracy: 0.9719\n",
      "Epoch 44/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20580/20580 [==============================] - 2s 92us/sample - loss: 0.0146 - sparse_categorical_accuracy: 0.9946 - val_loss: 0.1493 - val_sparse_categorical_accuracy: 0.9718\n",
      "Epoch 45/100\n",
      "20580/20580 [==============================] - 2s 93us/sample - loss: 0.0108 - sparse_categorical_accuracy: 0.9964 - val_loss: 0.1550 - val_sparse_categorical_accuracy: 0.9736\n",
      "Epoch 46/100\n",
      "20580/20580 [==============================] - 2s 90us/sample - loss: 0.0149 - sparse_categorical_accuracy: 0.9951 - val_loss: 0.1602 - val_sparse_categorical_accuracy: 0.9726\n",
      "Epoch 47/100\n",
      "20580/20580 [==============================] - 2s 94us/sample - loss: 0.0187 - sparse_categorical_accuracy: 0.9943 - val_loss: 0.1529 - val_sparse_categorical_accuracy: 0.9734\n",
      "Epoch 48/100\n",
      "20580/20580 [==============================] - 2s 89us/sample - loss: 0.0148 - sparse_categorical_accuracy: 0.9951 - val_loss: 0.1508 - val_sparse_categorical_accuracy: 0.9737\n",
      "Epoch 49/100\n",
      "20580/20580 [==============================] - 2s 89us/sample - loss: 0.0143 - sparse_categorical_accuracy: 0.9951 - val_loss: 0.1610 - val_sparse_categorical_accuracy: 0.9723\n",
      "Epoch 50/100\n",
      "20580/20580 [==============================] - 2s 89us/sample - loss: 0.0155 - sparse_categorical_accuracy: 0.9954 - val_loss: 0.1648 - val_sparse_categorical_accuracy: 0.9719\n",
      "Epoch 51/100\n",
      "20580/20580 [==============================] - 2s 90us/sample - loss: 0.0139 - sparse_categorical_accuracy: 0.9950 - val_loss: 0.1644 - val_sparse_categorical_accuracy: 0.9726\n",
      "Epoch 52/100\n",
      "20580/20580 [==============================] - 2s 89us/sample - loss: 0.0168 - sparse_categorical_accuracy: 0.9950 - val_loss: 0.1449 - val_sparse_categorical_accuracy: 0.9729\n",
      "Epoch 53/100\n",
      "20580/20580 [==============================] - 2s 91us/sample - loss: 0.0157 - sparse_categorical_accuracy: 0.9947 - val_loss: 0.1723 - val_sparse_categorical_accuracy: 0.9711\n",
      "Epoch 54/100\n",
      "20580/20580 [==============================] - 2s 88us/sample - loss: 0.0182 - sparse_categorical_accuracy: 0.9941 - val_loss: 0.1561 - val_sparse_categorical_accuracy: 0.9711\n",
      "Epoch 55/100\n",
      "20580/20580 [==============================] - 2s 88us/sample - loss: 0.0144 - sparse_categorical_accuracy: 0.9957 - val_loss: 0.1488 - val_sparse_categorical_accuracy: 0.9727\n",
      "Epoch 56/100\n",
      "20580/20580 [==============================] - 2s 89us/sample - loss: 0.0136 - sparse_categorical_accuracy: 0.9956 - val_loss: 0.1620 - val_sparse_categorical_accuracy: 0.9726\n",
      "Epoch 57/100\n",
      "20580/20580 [==============================] - 2s 92us/sample - loss: 0.0113 - sparse_categorical_accuracy: 0.9963 - val_loss: 0.1594 - val_sparse_categorical_accuracy: 0.9726\n",
      "Epoch 58/100\n",
      "20580/20580 [==============================] - 2s 91us/sample - loss: 0.0121 - sparse_categorical_accuracy: 0.9964 - val_loss: 0.1748 - val_sparse_categorical_accuracy: 0.9714\n",
      "Epoch 59/100\n",
      "20580/20580 [==============================] - 2s 88us/sample - loss: 0.0127 - sparse_categorical_accuracy: 0.9954 - val_loss: 0.1692 - val_sparse_categorical_accuracy: 0.9720\n",
      "Epoch 60/100\n",
      "20580/20580 [==============================] - 2s 90us/sample - loss: 0.0151 - sparse_categorical_accuracy: 0.9953 - val_loss: 0.1537 - val_sparse_categorical_accuracy: 0.9736\n",
      "Epoch 61/100\n",
      "20580/20580 [==============================] - 2s 90us/sample - loss: 0.0144 - sparse_categorical_accuracy: 0.9955 - val_loss: 0.1590 - val_sparse_categorical_accuracy: 0.9739\n",
      "Epoch 62/100\n",
      "20580/20580 [==============================] - 2s 88us/sample - loss: 0.0125 - sparse_categorical_accuracy: 0.9961 - val_loss: 0.1710 - val_sparse_categorical_accuracy: 0.9729\n",
      "Epoch 63/100\n",
      "20580/20580 [==============================] - 2s 90us/sample - loss: 0.0140 - sparse_categorical_accuracy: 0.9960 - val_loss: 0.1652 - val_sparse_categorical_accuracy: 0.9736\n",
      "Epoch 64/100\n",
      "20580/20580 [==============================] - 2s 88us/sample - loss: 0.0106 - sparse_categorical_accuracy: 0.9965 - val_loss: 0.1771 - val_sparse_categorical_accuracy: 0.9738\n",
      "Epoch 65/100\n",
      "20580/20580 [==============================] - 2s 90us/sample - loss: 0.0110 - sparse_categorical_accuracy: 0.9962 - val_loss: 0.1782 - val_sparse_categorical_accuracy: 0.9731\n",
      "Epoch 66/100\n",
      "20580/20580 [==============================] - 2s 87us/sample - loss: 0.0141 - sparse_categorical_accuracy: 0.9958 - val_loss: 0.1861 - val_sparse_categorical_accuracy: 0.9710\n",
      "Epoch 67/100\n",
      "20580/20580 [==============================] - 2s 90us/sample - loss: 0.0133 - sparse_categorical_accuracy: 0.9955 - val_loss: 0.1635 - val_sparse_categorical_accuracy: 0.9724\n",
      "Epoch 68/100\n",
      "20580/20580 [==============================] - 2s 89us/sample - loss: 0.0174 - sparse_categorical_accuracy: 0.9948 - val_loss: 0.1676 - val_sparse_categorical_accuracy: 0.9734\n",
      "Epoch 69/100\n",
      "20580/20580 [==============================] - 2s 90us/sample - loss: 0.0154 - sparse_categorical_accuracy: 0.9950 - val_loss: 0.1733 - val_sparse_categorical_accuracy: 0.9726\n",
      "Epoch 70/100\n",
      "20580/20580 [==============================] - 2s 90us/sample - loss: 0.0125 - sparse_categorical_accuracy: 0.9962 - val_loss: 0.1590 - val_sparse_categorical_accuracy: 0.9741\n",
      "Epoch 71/100\n",
      "20580/20580 [==============================] - 2s 90us/sample - loss: 0.0084 - sparse_categorical_accuracy: 0.9974 - val_loss: 0.1725 - val_sparse_categorical_accuracy: 0.9729\n",
      "Epoch 72/100\n",
      "20580/20580 [==============================] - 2s 88us/sample - loss: 0.0095 - sparse_categorical_accuracy: 0.9970 - val_loss: 0.1709 - val_sparse_categorical_accuracy: 0.9731\n",
      "Epoch 73/100\n",
      "20580/20580 [==============================] - 2s 89us/sample - loss: 0.0125 - sparse_categorical_accuracy: 0.9961 - val_loss: 0.1760 - val_sparse_categorical_accuracy: 0.9724\n",
      "Epoch 74/100\n",
      "20580/20580 [==============================] - 2s 89us/sample - loss: 0.0138 - sparse_categorical_accuracy: 0.9958 - val_loss: 0.1762 - val_sparse_categorical_accuracy: 0.9740\n",
      "Epoch 75/100\n",
      "20580/20580 [==============================] - 2s 92us/sample - loss: 0.0106 - sparse_categorical_accuracy: 0.9960 - val_loss: 0.1690 - val_sparse_categorical_accuracy: 0.9736\n",
      "Epoch 76/100\n",
      "20580/20580 [==============================] - 2s 91us/sample - loss: 0.0099 - sparse_categorical_accuracy: 0.9969 - val_loss: 0.1636 - val_sparse_categorical_accuracy: 0.9739\n",
      "Epoch 77/100\n",
      "20580/20580 [==============================] - 2s 88us/sample - loss: 0.0117 - sparse_categorical_accuracy: 0.9959 - val_loss: 0.1797 - val_sparse_categorical_accuracy: 0.9704\n",
      "Epoch 78/100\n",
      "20580/20580 [==============================] - 2s 87us/sample - loss: 0.0135 - sparse_categorical_accuracy: 0.9958 - val_loss: 0.1662 - val_sparse_categorical_accuracy: 0.9739\n",
      "Epoch 79/100\n",
      "20580/20580 [==============================] - 2s 87us/sample - loss: 0.0110 - sparse_categorical_accuracy: 0.9964 - val_loss: 0.1759 - val_sparse_categorical_accuracy: 0.9738\n",
      "Epoch 80/100\n",
      "20580/20580 [==============================] - 2s 91us/sample - loss: 0.0110 - sparse_categorical_accuracy: 0.9960 - val_loss: 0.1577 - val_sparse_categorical_accuracy: 0.9744\n",
      "Epoch 81/100\n",
      "20580/20580 [==============================] - 2s 87us/sample - loss: 0.0119 - sparse_categorical_accuracy: 0.9959 - val_loss: 0.1683 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 82/100\n",
      "20580/20580 [==============================] - 2s 88us/sample - loss: 0.0100 - sparse_categorical_accuracy: 0.9964 - val_loss: 0.1762 - val_sparse_categorical_accuracy: 0.9730\n",
      "Epoch 83/100\n",
      "20580/20580 [==============================] - 2s 89us/sample - loss: 0.0119 - sparse_categorical_accuracy: 0.9959 - val_loss: 0.1837 - val_sparse_categorical_accuracy: 0.9729\n",
      "Epoch 84/100\n",
      "20580/20580 [==============================] - 2s 87us/sample - loss: 0.0130 - sparse_categorical_accuracy: 0.9961 - val_loss: 0.1756 - val_sparse_categorical_accuracy: 0.9743\n",
      "Epoch 85/100\n",
      "20580/20580 [==============================] - 2s 89us/sample - loss: 0.0075 - sparse_categorical_accuracy: 0.9975 - val_loss: 0.1857 - val_sparse_categorical_accuracy: 0.9730\n",
      "Epoch 86/100\n",
      "20580/20580 [==============================] - 2s 98us/sample - loss: 0.0084 - sparse_categorical_accuracy: 0.9977 - val_loss: 0.1786 - val_sparse_categorical_accuracy: 0.9754\n",
      "Epoch 87/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20580/20580 [==============================] - 2s 89us/sample - loss: 0.0091 - sparse_categorical_accuracy: 0.9972 - val_loss: 0.1906 - val_sparse_categorical_accuracy: 0.9729\n",
      "Epoch 88/100\n",
      "20580/20580 [==============================] - 2s 88us/sample - loss: 0.0085 - sparse_categorical_accuracy: 0.9971 - val_loss: 0.1827 - val_sparse_categorical_accuracy: 0.9730\n",
      "Epoch 89/100\n",
      "20580/20580 [==============================] - 2s 89us/sample - loss: 0.0123 - sparse_categorical_accuracy: 0.9968 - val_loss: 0.2085 - val_sparse_categorical_accuracy: 0.9736\n",
      "Epoch 90/100\n",
      "20580/20580 [==============================] - 2s 90us/sample - loss: 0.0133 - sparse_categorical_accuracy: 0.9960 - val_loss: 0.1739 - val_sparse_categorical_accuracy: 0.9745\n",
      "Epoch 91/100\n",
      "20580/20580 [==============================] - 2s 89us/sample - loss: 0.0125 - sparse_categorical_accuracy: 0.9961 - val_loss: 0.1724 - val_sparse_categorical_accuracy: 0.9754\n",
      "Epoch 92/100\n",
      "20580/20580 [==============================] - 2s 89us/sample - loss: 0.0131 - sparse_categorical_accuracy: 0.9961 - val_loss: 0.1759 - val_sparse_categorical_accuracy: 0.9743\n",
      "Epoch 93/100\n",
      "20580/20580 [==============================] - 2s 89us/sample - loss: 0.0116 - sparse_categorical_accuracy: 0.9965 - val_loss: 0.1668 - val_sparse_categorical_accuracy: 0.9751\n",
      "Epoch 94/100\n",
      "20580/20580 [==============================] - 2s 89us/sample - loss: 0.0104 - sparse_categorical_accuracy: 0.9964 - val_loss: 0.1834 - val_sparse_categorical_accuracy: 0.9736\n",
      "Epoch 95/100\n",
      "20580/20580 [==============================] - 2s 89us/sample - loss: 0.0128 - sparse_categorical_accuracy: 0.9961 - val_loss: 0.1825 - val_sparse_categorical_accuracy: 0.9737\n",
      "Epoch 96/100\n",
      "20580/20580 [==============================] - 2s 88us/sample - loss: 0.0095 - sparse_categorical_accuracy: 0.9972 - val_loss: 0.1909 - val_sparse_categorical_accuracy: 0.9715\n",
      "Epoch 97/100\n",
      "20580/20580 [==============================] - 2s 89us/sample - loss: 0.0086 - sparse_categorical_accuracy: 0.9972 - val_loss: 0.1795 - val_sparse_categorical_accuracy: 0.9735\n",
      "Epoch 98/100\n",
      "20580/20580 [==============================] - 2s 89us/sample - loss: 0.0089 - sparse_categorical_accuracy: 0.9976 - val_loss: 0.1883 - val_sparse_categorical_accuracy: 0.9751\n",
      "Epoch 99/100\n",
      "20580/20580 [==============================] - 2s 89us/sample - loss: 0.0092 - sparse_categorical_accuracy: 0.9971 - val_loss: 0.1722 - val_sparse_categorical_accuracy: 0.9741\n",
      "Epoch 100/100\n",
      "20580/20580 [==============================] - 2s 89us/sample - loss: 0.0114 - sparse_categorical_accuracy: 0.9965 - val_loss: 0.1764 - val_sparse_categorical_accuracy: 0.9723\n"
     ]
    }
   ],
   "source": [
    "# Tensorflow 2.1\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Raw Data Loading\n",
    "df = pd.read_csv('./data/digit-recognizer/train.csv')\n",
    "\n",
    "# 결측치나 이상치는 없음\n",
    "# Feature Engineering 할 필요가 없음\n",
    "\n",
    "# Data Split\n",
    "x_data_train, x_data_test, t_data_train, t_data_test = \\\n",
    "train_test_split(df.drop('label', axis=1, inplace=False), df['label'], test_size=0.3, random_state=0)\n",
    "\n",
    "# Min-Max Normalization\n",
    "scaler = MinMaxScaler()   # scaler = StandardScaler()\n",
    "scaler.fit(x_data_train)\n",
    "x_data_train_norm = scaler.transform(x_data_train)\n",
    "x_data_test_norm = scaler.transform(x_data_test)\n",
    "\n",
    "# TF 2.1 구현\n",
    "model = Sequential()\n",
    "model.add(Dense(256, activation='relu', kernel_initializer='he_uniform',\n",
    "                input_shape=(x_data_train_norm.shape[1],)))  # input_shape로 input layer의 역할까지도 처리\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(10, activation='softmax', kernel_initializer='he_uniform'))\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=1e-3),\n",
    "              loss='sparse_categorical_crossentropy',   # sparse를 붙여주면 one-hot encoing 작업을 생략 가능\n",
    "              metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "history = model.fit(x_data_train_norm,\n",
    "                    t_data_train,\n",
    "                    epochs=100,\n",
    "                    batch_size=100,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98      1242\n",
      "           1       0.98      0.99      0.99      1429\n",
      "           2       0.98      0.98      0.98      1276\n",
      "           3       0.97      0.98      0.97      1298\n",
      "           4       0.98      0.97      0.98      1236\n",
      "           5       0.97      0.97      0.97      1119\n",
      "           6       0.97      0.99      0.98      1243\n",
      "           7       0.98      0.97      0.97      1334\n",
      "           8       0.97      0.96      0.96      1204\n",
      "           9       0.97      0.96      0.97      1219\n",
      "\n",
      "    accuracy                           0.98     12600\n",
      "   macro avg       0.98      0.98      0.98     12600\n",
      "weighted avg       0.98      0.98      0.98     12600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = np.argmax(model.predict(x_data_test_norm), axis=1)\n",
    "print(classification_report(t_data_test, result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'sparse_categorical_accuracy', 'val_loss', 'val_sparse_categorical_accuracy'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(history.history.keys())\n",
    "plt.plot(history.history['sparse_categorical_accuracy'], color='r')\n",
    "plt.plot(history.history['val_sparse_categorical_accuracy'], color='b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data_env_tensorflow2]",
   "language": "python",
   "name": "conda-env-data_env_tensorflow2-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
