{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "# Raw Data Loading\n",
    "train = pd.read_csv('./data/digit-recognizer/train.csv')\n",
    "test = pd.read_csv('./data/digit-recognizer/test.csv')\n",
    "\n",
    "# # Data Split\n",
    "# # 7:3 비율로 train과 test 분리\n",
    "# # x_data_test, t_data_test는 맨끝에서 모델의 최종 Accuracy를 측정할 때 딱 한번 사용\n",
    "# x_data_train, x_data_test, t_data_train, t_data_test = \\\n",
    "# train_test_split(df[['height','weight']], df['label'], test_size=0.3, random_state=0)\n",
    "\n",
    "# # Normalization\n",
    "# scaler = MinMaxScaler()\n",
    "# scaler.fit(x_data_train)  # scaling을 하기 위한 정보를 scaler에 setting\n",
    "\n",
    "# x_data_train_norm = scaler.transform(x_data_train)\n",
    "# x_data_test_norm = scaler.transform(x_data_test)\n",
    "\n",
    "# # Tensorflow 구현\n",
    "# # 해당 데이터는 Multinomial이기 때문에 One-Hot Encoding일 이용해 데이터 변환 필요\n",
    "# # 0 => 1 0 0\n",
    "# # 1 => 0 1 0\n",
    "# # 2 => 0 0 1\n",
    "# sess = tf.Session()\n",
    "# t_data_train_onehot = sess.run(tf.one_hot(t_data_train, depth=3))\n",
    "# t_data_test_onehot = sess.run(tf.one_hot(t_data_test, depth=3))\n",
    "\n",
    "# # Placeholder\n",
    "# X = tf.placeholder(shape=[None,2], dtype=tf.float32)\n",
    "# T = tf.placeholder(shape=[None,3], dtype=tf.float32)\n",
    "\n",
    "# # Weight & bias\n",
    "# W = tf.Variable(tf.random.normal([2,3]), name='weight')\n",
    "# b = tf.Variable(tf.random.normal([3]), name='bias')\n",
    "\n",
    "# # Hypothesis\n",
    "# logit = tf.matmul(X,W) + b\n",
    "# H = tf.nn.softmax(logit)\n",
    "\n",
    "# # Loss Fucntion\n",
    "# loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=T))\n",
    "\n",
    "# # Train\n",
    "# train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(loss)\n",
    "\n",
    "# # 반복학습\n",
    "# # parameter\n",
    "# num_of_epoch = 1000\n",
    "# batch_size = 100     # 한번에 학습할 x_data와 t_data의 행의 수\n",
    "\n",
    "# def run_train(sess, train_x, train_t):\n",
    "#     print('======= 학습시작 =======')\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     total_batch = int(train_x.shape[0] / batch_size)\n",
    "    \n",
    "#     for step in range(num_of_epoch):\n",
    "#         for i in range(total_batch):\n",
    "#             batch_x = train_x[i*batch_size:(i+1)*batch_size]\n",
    "#             batch_t = train_t[i*batch_size:(i+1)*batch_size]\n",
    "#             _, loss_val = sess.run([train,loss], feed_dict={X : batch_x, T : batch_t})\n",
    "\n",
    "#         if step % 100 == 0 :\n",
    "#             print('Loss : {}'.format(loss_val))\n",
    "#     print('======= 학습종료 =======')\n",
    "    \n",
    "# # Evaluation(Accuracy)\n",
    "# predict = tf.argmax(H,1)  # 1 => axis를 의미\n",
    "# correct = tf.equal(predict, tf.argmax(T,1))\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "\n",
    "# # Cross Validation\n",
    "# cv = 5        # KFold의 K\n",
    "# results = []   # 각 set에서 구한 accuracy를 집어넣을 list\n",
    "# kf = KFold(n_splits=cv, shuffle=True)\n",
    "\n",
    "# for training_idx, validation_idx in kf.split(x_data_train_norm):\n",
    "#     train_x = x_data_train_norm[training_idx]\n",
    "#     train_t = t_data_train_onehot[training_idx]\n",
    "#     valid_x = x_data_train_norm[validation_idx]\n",
    "#     valid_t = t_data_train_onehot[validation_idx]\n",
    "\n",
    "#     run_train(sess, train_x, train_t)\n",
    "#     results.append(sess.run(accuracy, feed_dict={X: valid_x, T : valid_t}))\n",
    "\n",
    "# print('Cross Validation 결과 : {}'.format(np.mean(results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Split\n",
    "x_data_train, x_data_test, t_data_train, t_data_test = \\\n",
    "train_test_split(train.iloc[:,1:], train['label'], test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Normalization\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x_data_train)  # scaling을 하기 위한 정보를 scaler에 setting\n",
    "\n",
    "x_data_train_norm = scaler.transform(x_data_train)\n",
    "x_data_test_norm = scaler.transform(x_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "t_data_train_onehot = sess.run(tf.one_hot(t_data_train, depth=10))\n",
    "t_data_test_onehot = sess.run(tf.one_hot(t_data_test, depth=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder\n",
    "X = tf.placeholder(shape=[None,784], dtype=tf.float32)\n",
    "T = tf.placeholder(shape=[None,10], dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "W = tf.Variable(tf.random.normal([784,10]), name='weight')\n",
    "b = tf.Variable(tf.random.normal([10]), name='bias')\n",
    "\n",
    "# Hypothesis\n",
    "logit = tf.matmul(X,W) + b\n",
    "H = tf.nn.softmax(logit)\n",
    "\n",
    "# Loss Fucntion\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=T))\n",
    "\n",
    "# Train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(loss)\n",
    "\n",
    "# 반복학습\n",
    "# parameter\n",
    "num_of_epoch = 1000\n",
    "batch_size = 100     # 한번에 학습할 x_data와 t_data의 행의 수\n",
    "\n",
    "def run_train(sess, train_x, train_t):\n",
    "    print('======= 학습시작 =======')\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    total_batch = int(train_x.shape[0] / batch_size)\n",
    "    \n",
    "    for step in range(num_of_epoch):\n",
    "        for i in range(total_batch):\n",
    "            batch_x = train_x[i*batch_size:(i+1)*batch_size]\n",
    "            batch_t = train_t[i*batch_size:(i+1)*batch_size]\n",
    "            _, loss_val = sess.run([train,loss], feed_dict={X : batch_x, T : batch_t})\n",
    "\n",
    "        if step % 100 == 0 :\n",
    "            print('Loss : {}'.format(loss_val))\n",
    "    print('======= 학습종료 =======')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation(Accuracy)\n",
    "predict = tf.argmax(H,1)  # 1 => axis를 의미\n",
    "correct = tf.equal(predict, tf.argmax(T,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "\n",
    "# Cross Validation\n",
    "cv = 5        # KFold의 K\n",
    "results = []   # 각 set에서 구한 accuracy를 집어넣을 list\n",
    "kf = KFold(n_splits=cv, shuffle=True)\n",
    "\n",
    "for training_idx, validation_idx in kf.split(x_data_train_norm):\n",
    "    train_x = x_data_train_norm[training_idx]\n",
    "    train_t = t_data_train_onehot[training_idx]\n",
    "    valid_x = x_data_train_norm[validation_idx]\n",
    "    valid_t = t_data_train_onehot[validation_idx]\n",
    "\n",
    "    run_train(sess, train_x, train_t)\n",
    "    results.append(sess.run(accuracy, feed_dict={X: valid_x, T : valid_t}))\n",
    "\n",
    "print('Cross Validation 결과 : {}'.format(np.mean(results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습진행\n",
    "run_train(sess, x_data_train_norm, t_data_train_onehot)\n",
    "\n",
    "# Accuracy 측정\n",
    "result = sess.run(accuracy, feed_dict={X: x_data_test_norm, T : t_data_test_onehot})\n",
    "print('최종 Accuracy : {}'.format(result))\n",
    "\n",
    "# Prediction\n",
    "test_scaled = scaler.transform(test)\n",
    "\n",
    "result = sess.run(H, feed_dict={X : test_scaled})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.argmax(result[0]))\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "imageid = list(range(1,result.shape[0]+1))\n",
    "label = []\n",
    "\n",
    "for i in range(result.shape[0]):\n",
    "    label.append(np.argmax(result[i]))\n",
    "    \n",
    "df['ImageId'] = imageid\n",
    "df['Label'] = label\n",
    "\n",
    "df.to_csv('./submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2020/10/16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset\n",
    "\n",
    "# Tensorflow 1.15버전\n",
    "# multinomial classification으로 MNIST 구현\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler        # Normalization\n",
    "from sklearn.model_selection import train_test_split  # train, test 분리\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Raw Data Loading\n",
    "df = pd.read_csv('./data/mnist/train.csv')\n",
    "\n",
    "##### 결측치와 이상치는 없음 #####\n",
    "\n",
    "##### 이미지 확인 #####\n",
    "img_data = df.drop('label', axis=1, inplace=False).values\n",
    "\n",
    "fig = plt.figure()\n",
    "fig_arr = []\n",
    "\n",
    "for n in range(10):\n",
    "    fig_arr.append(fig.add_subplot(2,5,n+1))\n",
    "    fig_arr[n].imshow(img_data[n].reshape(28,28), cmap='Greys')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Data Split\n",
    "x_data_train, x_data_test, t_data_train, t_data_test = \\\n",
    "train_test_split(df.drop('label', axis=1, inplace=False), df['label'], test_size=0.3, random_state=0)\n",
    "\n",
    "# Min-Max Normalization\n",
    "scaler = MinMaxScaler()   # scaler = StandardScaler()\n",
    "scaler.fit(x_data_train)\n",
    "x_data_train_norm = scaler.transform(x_data_train)\n",
    "x_data_test_norm = scaler.transform(x_data_test)\n",
    "\n",
    "del x_data_train\n",
    "del x_data_test\n",
    "\n",
    "##### Tensorflow implementation #####\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "t_data_train_onehot = sess.run(tf.one_hot(t_data_train,depth=10))\n",
    "t_data_test_onehot = sess.run(tf.one_hot(t_data_test,depth=10))\n",
    "\n",
    "# Placeholder\n",
    "X = tf.placeholder(shape=[None,784], dtype=tf.float32)\n",
    "T = tf.placeholder(shape=[None,10], dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "W = tf.Variable(tf.random.normal([784,10]), name='weight')\n",
    "b = tf.Variable(tf.random.normal([10]), name='bias')\n",
    "\n",
    "# Hypothesis\n",
    "logit = tf.matmul(X,W) + b\n",
    "H = tf.nn.softmax(logit)   # softmax activation function\n",
    "\n",
    "# loss\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, \n",
    "                                                                 labels=T))\n",
    "\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-1).minimize(loss)\n",
    "\n",
    "# parameter\n",
    "num_of_epoch = 200\n",
    "batch_size = 100\n",
    "\n",
    "\n",
    "# 학습\n",
    "def run_train(sess, train_x, train_t):\n",
    "    print('### Starting Training ###')\n",
    "    # 초기화\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(num_of_epoch):\n",
    "        total_batch = int(train_x.shape[0] / batch_size)\n",
    "\n",
    "        for i in range(total_batch):\n",
    "            batch_x = train_x[i*batch_size:(i+1)*batch_size]\n",
    "            batch_t = train_t[i*batch_size:(i+1)*batch_size]\n",
    "            _, loss_val = sess.run([train,loss], feed_dict={X:batch_x, T:batch_t})\n",
    "\n",
    "        if step % 20 == 0:\n",
    "            print('Loss : {}'.format(loss_val))\n",
    "    print('### End Training ###')\n",
    "\n",
    "    \n",
    "# Accuracy\n",
    "predict = tf.argmax(H,1)\n",
    "\n",
    "   \n",
    "# Testing\n",
    "run_train(sess,x_data_train_norm,t_data_train_onehot) # 학습\n",
    "print('### Test Set으로 Accuracy 측정 ###')\n",
    "result = sess.run(predict, feed_dict={X:x_data_test_norm})\n",
    "print(classification_report(t_data_test,result.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset\n",
    "\n",
    "# Tensorflow 1.15버전\n",
    "# DNN(초기화, activation function, dropout처리 안함)으로 MNIST 구현\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler        # Normalization\n",
    "from sklearn.model_selection import train_test_split  # train, test 분리\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Raw Data Loading\n",
    "df = pd.read_csv('./data/digit-recognizer/train.csv')\n",
    "display(df.head(), df.shape)\n",
    "\n",
    "##### 결측치와 이상치는 없음 #####\n",
    "\n",
    "# Data Split\n",
    "x_data_train, x_data_test, t_data_train, t_data_test = \\\n",
    "train_test_split(df.drop('label', axis=1, inplace=False), df['label'], test_size=0.3, random_state=0)\n",
    "# test_size : test set의 비율 (0.3 => 30%)\n",
    "# random_state : split할 때 랜덤하게 split하게 되는데 이를 일정하게 고정(seed의 개념)\n",
    "\n",
    "# Min-Max Normalization\n",
    "scaler = MinMaxScaler()   # scaler = StandardScaler()\n",
    "scaler.fit(x_data_train)\n",
    "x_data_train_norm = scaler.transform(x_data_train)\n",
    "x_data_test_norm = scaler.transform(x_data_test)\n",
    "\n",
    "del x_data_train\n",
    "del x_data_test\n",
    "\n",
    "##### Tensorflow implementation #####\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "t_data_train_onehot = sess.run(tf.one_hot(t_data_train,depth=10))\n",
    "t_data_test_onehot = sess.run(tf.one_hot(t_data_test,depth=10))\n",
    "\n",
    "# del t_data_train\n",
    "# del t_data_test\n",
    "\n",
    "# Placeholder\n",
    "X = tf.placeholder(shape=[None,784], dtype=tf.float32)\n",
    "T = tf.placeholder(shape=[None,10], dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "W2 = tf.Variable(tf.random.normal([784,256]), name='weight2')\n",
    "b2 = tf.Variable(tf.random.normal([256]), name='bias2')\n",
    "layer2 = tf.sigmoid(tf.matmul(X,W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random.normal([256,128]), name='weight3')\n",
    "b3 = tf.Variable(tf.random.normal([128]), name='bias3')\n",
    "layer3 = tf.sigmoid(tf.matmul(layer2,W3) + b3)\n",
    "\n",
    "W4 = tf.Variable(tf.random.normal([128,10]), name='weight4')\n",
    "b4 = tf.Variable(tf.random.normal([10]), name='bias4')\n",
    "\n",
    "# Hypothesis\n",
    "logit = tf.matmul(layer3,W4) + b4\n",
    "H = tf.nn.softmax(logit)   # softmax activation function\n",
    "\n",
    "# loss\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, \n",
    "                                                                 labels=T))\n",
    "\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-1).minimize(loss)\n",
    "\n",
    "# parameter\n",
    "num_of_epoch = 500\n",
    "batch_size = 100\n",
    "\n",
    "\n",
    "# 학습\n",
    "def run_train(sess, train_x, train_t):\n",
    "    print('### Starting Training ###')\n",
    "    # 초기화\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(num_of_epoch):\n",
    "        total_batch = int(train_x.shape[0] / batch_size)\n",
    "\n",
    "        for i in range(total_batch):\n",
    "            batch_x = train_x[i*batch_size:(i+1)*batch_size]\n",
    "            batch_t = train_t[i*batch_size:(i+1)*batch_size]\n",
    "            _, loss_val = sess.run([train,loss], feed_dict={X:batch_x, T:batch_t})\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            print('Loss : {}'.format(loss_val))\n",
    "    print('### End Training ###')\n",
    "\n",
    "    \n",
    "# Accuracy\n",
    "predict = tf.argmax(H,1)\n",
    "correct = tf.equal(predict, tf.argmax(T,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "\n",
    "# Testing\n",
    "run_train(sess,x_data_train_norm,t_data_train_onehot) # 학습\n",
    "print('### Test Set으로 Accuracy 측정 ###')\n",
    "result = sess.run(predict, feed_dict={X:x_data_test_norm})\n",
    "print(classification_report(t_data_test,result.ravel()))\n",
    "# Test Set 정확도 : 0.93"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(42000, 785)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "### Starting Training ###\n",
      "Loss : 0.2679951786994934\n",
      "Loss : 0.0017695678398013115\n",
      "Loss : 0.0005366549012251198\n",
      "Loss : 0.00030197089654393494\n",
      "Loss : 0.00020804045198019594\n",
      "Loss : 0.00015697623894084245\n",
      "Loss : 0.00012554744898807257\n",
      "Loss : 0.000103945450973697\n",
      "Loss : 8.81961896084249e-05\n",
      "Loss : 7.647493475815281e-05\n",
      "### End Training ###\n",
      "### Test Set으로 Accuracy 측정 ###\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98      1242\n",
      "           1       0.98      0.99      0.99      1429\n",
      "           2       0.97      0.98      0.97      1276\n",
      "           3       0.97      0.96      0.97      1298\n",
      "           4       0.98      0.97      0.97      1236\n",
      "           5       0.96      0.97      0.96      1119\n",
      "           6       0.98      0.99      0.98      1243\n",
      "           7       0.98      0.97      0.97      1334\n",
      "           8       0.96      0.96      0.96      1204\n",
      "           9       0.96      0.96      0.96      1219\n",
      "\n",
      "    accuracy                           0.97     12600\n",
      "   macro avg       0.97      0.97      0.97     12600\n",
      "weighted avg       0.97      0.97      0.97     12600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "\n",
    "# Tensorflow 1.15버전\n",
    "# DNN(dropout처리 안함)으로 MNIST 구현\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler        # Normalization\n",
    "from sklearn.model_selection import train_test_split  # train, test 분리\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Raw Data Loading\n",
    "df = pd.read_csv('./data/digit-recognizer/train.csv')\n",
    "display(df.head(), df.shape)\n",
    "\n",
    "##### 결측치와 이상치는 없음 #####\n",
    "\n",
    "# Data Split\n",
    "x_data_train, x_data_test, t_data_train, t_data_test = \\\n",
    "train_test_split(df.drop('label', axis=1, inplace=False), df['label'], test_size=0.3, random_state=0)\n",
    "# test_size : test set의 비율 (0.3 => 30%)\n",
    "# random_state : split할 때 랜덤하게 split하게 되는데 이를 일정하게 고정(seed의 개념)\n",
    "\n",
    "# Min-Max Normalization\n",
    "scaler = MinMaxScaler()   # scaler = StandardScaler()\n",
    "scaler.fit(x_data_train)\n",
    "x_data_train_norm = scaler.transform(x_data_train)\n",
    "x_data_test_norm = scaler.transform(x_data_test)\n",
    "\n",
    "del x_data_train\n",
    "del x_data_test\n",
    "\n",
    "##### Tensorflow implementation #####\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "t_data_train_onehot = sess.run(tf.one_hot(t_data_train,depth=10))\n",
    "t_data_test_onehot = sess.run(tf.one_hot(t_data_test,depth=10))\n",
    "\n",
    "# del t_data_train\n",
    "# del t_data_test\n",
    "\n",
    "# Placeholder\n",
    "X = tf.placeholder(shape=[None,784], dtype=tf.float32)\n",
    "T = tf.placeholder(shape=[None,10], dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "# He's 초기화\n",
    "W2 = tf.get_variable('w2', shape=[784,256], initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "b2 = tf.Variable(tf.random.normal([256]), name='bias2')\n",
    "layer2 = tf.nn.relu(tf.matmul(X,W2) + b2)\n",
    "\n",
    "W3 = tf.get_variable('w3', shape=[256,128], initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "b3 = tf.Variable(tf.random.normal([128]), name='bias3')\n",
    "layer3 = tf.nn.relu(tf.matmul(layer2,W3) + b3)\n",
    "\n",
    "W4 = tf.get_variable('w4', shape=[128,10], initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "b4 = tf.Variable(tf.random.normal([10]), name='bias4')\n",
    "\n",
    "# Hypothesis\n",
    "logit = tf.matmul(layer3,W4) + b4\n",
    "H = tf.nn.softmax(logit)   # softmax activation function\n",
    "\n",
    "# loss\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, \n",
    "                                                                 labels=T))\n",
    "\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-1).minimize(loss)\n",
    "\n",
    "# parameter\n",
    "num_of_epoch = 500\n",
    "batch_size = 100\n",
    "\n",
    "\n",
    "# 학습\n",
    "def run_train(sess, train_x, train_t):\n",
    "    print('### Starting Training ###')\n",
    "    # 초기화\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(num_of_epoch):\n",
    "        total_batch = int(train_x.shape[0] / batch_size)\n",
    "\n",
    "        for i in range(total_batch):\n",
    "            batch_x = train_x[i*batch_size:(i+1)*batch_size]\n",
    "            batch_t = train_t[i*batch_size:(i+1)*batch_size]\n",
    "            _, loss_val = sess.run([train,loss], feed_dict={X:batch_x, T:batch_t})\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            print('Loss : {}'.format(loss_val))\n",
    "    print('### End Training ###')\n",
    "\n",
    "    \n",
    "# Accuracy\n",
    "predict = tf.argmax(H,1)\n",
    "correct = tf.equal(predict, tf.argmax(T,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "\n",
    "# Testing\n",
    "run_train(sess,x_data_train_norm,t_data_train_onehot) # 학습\n",
    "print('### Test Set으로 Accuracy 측정 ###')\n",
    "result = sess.run(predict, feed_dict={X:x_data_test_norm})\n",
    "print(classification_report(t_data_test,result.ravel()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data_env] *",
   "language": "python",
   "name": "conda-env-data_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
