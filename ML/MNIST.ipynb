{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "# Raw Data Loading\n",
    "train = pd.read_csv('./data/digit-recognizer/train.csv')\n",
    "test = pd.read_csv('./data/digit-recognizer/test.csv')\n",
    "\n",
    "# # Data Split\n",
    "# # 7:3 비율로 train과 test 분리\n",
    "# # x_data_test, t_data_test는 맨끝에서 모델의 최종 Accuracy를 측정할 때 딱 한번 사용\n",
    "# x_data_train, x_data_test, t_data_train, t_data_test = \\\n",
    "# train_test_split(df[['height','weight']], df['label'], test_size=0.3, random_state=0)\n",
    "\n",
    "# # Normalization\n",
    "# scaler = MinMaxScaler()\n",
    "# scaler.fit(x_data_train)  # scaling을 하기 위한 정보를 scaler에 setting\n",
    "\n",
    "# x_data_train_norm = scaler.transform(x_data_train)\n",
    "# x_data_test_norm = scaler.transform(x_data_test)\n",
    "\n",
    "# # Tensorflow 구현\n",
    "# # 해당 데이터는 Multinomial이기 때문에 One-Hot Encoding일 이용해 데이터 변환 필요\n",
    "# # 0 => 1 0 0\n",
    "# # 1 => 0 1 0\n",
    "# # 2 => 0 0 1\n",
    "# sess = tf.Session()\n",
    "# t_data_train_onehot = sess.run(tf.one_hot(t_data_train, depth=3))\n",
    "# t_data_test_onehot = sess.run(tf.one_hot(t_data_test, depth=3))\n",
    "\n",
    "# # Placeholder\n",
    "# X = tf.placeholder(shape=[None,2], dtype=tf.float32)\n",
    "# T = tf.placeholder(shape=[None,3], dtype=tf.float32)\n",
    "\n",
    "# # Weight & bias\n",
    "# W = tf.Variable(tf.random.normal([2,3]), name='weight')\n",
    "# b = tf.Variable(tf.random.normal([3]), name='bias')\n",
    "\n",
    "# # Hypothesis\n",
    "# logit = tf.matmul(X,W) + b\n",
    "# H = tf.nn.softmax(logit)\n",
    "\n",
    "# # Loss Fucntion\n",
    "# loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=T))\n",
    "\n",
    "# # Train\n",
    "# train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(loss)\n",
    "\n",
    "# # 반복학습\n",
    "# # parameter\n",
    "# num_of_epoch = 1000\n",
    "# batch_size = 100     # 한번에 학습할 x_data와 t_data의 행의 수\n",
    "\n",
    "# def run_train(sess, train_x, train_t):\n",
    "#     print('======= 학습시작 =======')\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     total_batch = int(train_x.shape[0] / batch_size)\n",
    "    \n",
    "#     for step in range(num_of_epoch):\n",
    "#         for i in range(total_batch):\n",
    "#             batch_x = train_x[i*batch_size:(i+1)*batch_size]\n",
    "#             batch_t = train_t[i*batch_size:(i+1)*batch_size]\n",
    "#             _, loss_val = sess.run([train,loss], feed_dict={X : batch_x, T : batch_t})\n",
    "\n",
    "#         if step % 100 == 0 :\n",
    "#             print('Loss : {}'.format(loss_val))\n",
    "#     print('======= 학습종료 =======')\n",
    "    \n",
    "# # Evaluation(Accuracy)\n",
    "# predict = tf.argmax(H,1)  # 1 => axis를 의미\n",
    "# correct = tf.equal(predict, tf.argmax(T,1))\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "\n",
    "# # Cross Validation\n",
    "# cv = 5        # KFold의 K\n",
    "# results = []   # 각 set에서 구한 accuracy를 집어넣을 list\n",
    "# kf = KFold(n_splits=cv, shuffle=True)\n",
    "\n",
    "# for training_idx, validation_idx in kf.split(x_data_train_norm):\n",
    "#     train_x = x_data_train_norm[training_idx]\n",
    "#     train_t = t_data_train_onehot[training_idx]\n",
    "#     valid_x = x_data_train_norm[validation_idx]\n",
    "#     valid_t = t_data_train_onehot[validation_idx]\n",
    "\n",
    "#     run_train(sess, train_x, train_t)\n",
    "#     results.append(sess.run(accuracy, feed_dict={X: valid_x, T : valid_t}))\n",
    "\n",
    "# print('Cross Validation 결과 : {}'.format(np.mean(results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Split\n",
    "x_data_train, x_data_test, t_data_train, t_data_test = \\\n",
    "train_test_split(train.iloc[:,1:], train['label'], test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Normalization\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x_data_train)  # scaling을 하기 위한 정보를 scaler에 setting\n",
    "\n",
    "x_data_train_norm = scaler.transform(x_data_train)\n",
    "x_data_test_norm = scaler.transform(x_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "t_data_train_onehot = sess.run(tf.one_hot(t_data_train, depth=10))\n",
    "t_data_test_onehot = sess.run(tf.one_hot(t_data_test, depth=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder\n",
    "X = tf.placeholder(shape=[None,784], dtype=tf.float32)\n",
    "T = tf.placeholder(shape=[None,10], dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "W = tf.Variable(tf.random.normal([784,10]), name='weight')\n",
    "b = tf.Variable(tf.random.normal([10]), name='bias')\n",
    "\n",
    "# Hypothesis\n",
    "logit = tf.matmul(X,W) + b\n",
    "H = tf.nn.softmax(logit)\n",
    "\n",
    "# Loss Fucntion\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=T))\n",
    "\n",
    "# Train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(loss)\n",
    "\n",
    "# 반복학습\n",
    "# parameter\n",
    "num_of_epoch = 1000\n",
    "batch_size = 100     # 한번에 학습할 x_data와 t_data의 행의 수\n",
    "\n",
    "def run_train(sess, train_x, train_t):\n",
    "    print('======= 학습시작 =======')\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    total_batch = int(train_x.shape[0] / batch_size)\n",
    "    \n",
    "    for step in range(num_of_epoch):\n",
    "        for i in range(total_batch):\n",
    "            batch_x = train_x[i*batch_size:(i+1)*batch_size]\n",
    "            batch_t = train_t[i*batch_size:(i+1)*batch_size]\n",
    "            _, loss_val = sess.run([train,loss], feed_dict={X : batch_x, T : batch_t})\n",
    "\n",
    "        if step % 100 == 0 :\n",
    "            print('Loss : {}'.format(loss_val))\n",
    "    print('======= 학습종료 =======')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= 학습시작 =======\n",
      "Loss : 2.4283809661865234\n",
      "Loss : 0.2629198133945465\n",
      "Loss : 0.16819070279598236\n",
      "Loss : 0.13415943086147308\n",
      "Loss : 0.11908906698226929\n",
      "Loss : 0.11135406792163849\n",
      "Loss : 0.10683950781822205\n",
      "Loss : 0.10393599420785904\n",
      "Loss : 0.10193467885255814\n",
      "Loss : 0.1004893034696579\n",
      "======= 학습종료 =======\n",
      "======= 학습시작 =======\n",
      "Loss : 2.8176560401916504\n",
      "Loss : 0.22051656246185303\n",
      "Loss : 0.186706081032753\n",
      "Loss : 0.1710643321275711\n",
      "Loss : 0.16140617430210114\n",
      "Loss : 0.1547219604253769\n",
      "Loss : 0.1497219204902649\n",
      "Loss : 0.14578860998153687\n",
      "Loss : 0.14258462190628052\n",
      "Loss : 0.13989873230457306\n",
      "======= 학습종료 =======\n",
      "======= 학습시작 =======\n",
      "Loss : 1.9570811986923218\n",
      "Loss : 0.2173765003681183\n",
      "Loss : 0.18557173013687134\n",
      "Loss : 0.16982045769691467\n",
      "Loss : 0.1587662398815155\n",
      "Loss : 0.1502857804298401\n",
      "Loss : 0.1435655951499939\n",
      "Loss : 0.13803061842918396\n",
      "Loss : 0.1332874447107315\n",
      "Loss : 0.12909863889217377\n",
      "======= 학습종료 =======\n",
      "======= 학습시작 =======\n",
      "Loss : 2.003420352935791\n",
      "Loss : 0.2533223628997803\n",
      "Loss : 0.2093057781457901\n",
      "Loss : 0.18868908286094666\n",
      "Loss : 0.17707736790180206\n",
      "Loss : 0.1701645702123642\n",
      "Loss : 0.16579300165176392\n",
      "Loss : 0.1628100574016571\n",
      "Loss : 0.1605667620897293\n",
      "Loss : 0.1586948037147522\n",
      "======= 학습종료 =======\n",
      "======= 학습시작 =======\n",
      "Loss : 1.8849600553512573\n",
      "Loss : 0.15630844235420227\n",
      "Loss : 0.11568022519350052\n",
      "Loss : 0.10415142774581909\n",
      "Loss : 0.09952152520418167\n",
      "Loss : 0.09766210615634918\n",
      "Loss : 0.0969320610165596\n",
      "Loss : 0.09662985056638718\n",
      "Loss : 0.09646429866552353\n",
      "Loss : 0.09632101655006409\n",
      "======= 학습종료 =======\n",
      "Cross Validation 결과 : 0.9068366885185242\n"
     ]
    }
   ],
   "source": [
    "# Evaluation(Accuracy)\n",
    "predict = tf.argmax(H,1)  # 1 => axis를 의미\n",
    "correct = tf.equal(predict, tf.argmax(T,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "\n",
    "# Cross Validation\n",
    "cv = 5        # KFold의 K\n",
    "results = []   # 각 set에서 구한 accuracy를 집어넣을 list\n",
    "kf = KFold(n_splits=cv, shuffle=True)\n",
    "\n",
    "for training_idx, validation_idx in kf.split(x_data_train_norm):\n",
    "    train_x = x_data_train_norm[training_idx]\n",
    "    train_t = t_data_train_onehot[training_idx]\n",
    "    valid_x = x_data_train_norm[validation_idx]\n",
    "    valid_t = t_data_train_onehot[validation_idx]\n",
    "\n",
    "    run_train(sess, train_x, train_t)\n",
    "    results.append(sess.run(accuracy, feed_dict={X: valid_x, T : valid_t}))\n",
    "\n",
    "print('Cross Validation 결과 : {}'.format(np.mean(results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= 학습시작 =======\n",
      "Loss : 1.5984625816345215\n",
      "Loss : 0.21775935590267181\n",
      "Loss : 0.17829474806785583\n",
      "Loss : 0.16115707159042358\n",
      "Loss : 0.15355142951011658\n",
      "Loss : 0.14965103566646576\n",
      "Loss : 0.14720752835273743\n",
      "Loss : 0.1453894078731537\n",
      "Loss : 0.14386913180351257\n",
      "Loss : 0.1425083726644516\n",
      "======= 학습종료 =======\n",
      "최종 Accuracy : 0.9137301445007324\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "# 학습진행\n",
    "run_train(sess, x_data_train_norm, t_data_train_onehot)\n",
    "\n",
    "# Accuracy 측정\n",
    "result = sess.run(accuracy, feed_dict={X: x_data_test_norm, T : t_data_test_onehot})\n",
    "print('최종 Accuracy : {}'.format(result))\n",
    "\n",
    "# Prediction\n",
    "test_scaled = scaler.transform(test)\n",
    "\n",
    "result = sess.run(H, feed_dict={X : test_scaled})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "(28000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(np.argmax(result[0]))\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "imageid = list(range(1,result.shape[0]+1))\n",
    "label = []\n",
    "\n",
    "for i in range(result.shape[0]):\n",
    "    label.append(np.argmax(result[i]))\n",
    "    \n",
    "df['ImageId'] = imageid\n",
    "df['Label'] = label\n",
    "\n",
    "df.to_csv('./submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data_env] *",
   "language": "python",
   "name": "conda-env-data_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
