{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi Variable Logistic Regression\n",
    "# 내 성적 [600, 3.8, 1.]으로 진학가능할지 여부\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import linear_model\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 수치 미분함수\n",
    "from my_library.machine_learning_library import numerical_derivative\n",
    "\n",
    "# Raw Data Loading\n",
    "df = pd.read_csv('./data/admission.csv')\n",
    "# Data Preprocessing\n",
    "# 결측치 확인\n",
    "# print(df.isnull().sum()) 결측치 존재 x\n",
    "\n",
    "# 만약 있었으면 결측치 제거\n",
    "# df = df.dropna(how='any')\n",
    "\n",
    "# 이상치 처리\n",
    "# Z-score 이용\n",
    "zscore_threshold = 2.0\n",
    "\n",
    "for col in df.columns:\n",
    "    outliers = df[col][np.abs(stats.zscore(df[col])) > zscore_threshold]\n",
    "    df = df.loc[~df[col].isin(outliers)]\n",
    "\n",
    "# 정규화\n",
    "# MinMaxScaler 이용\n",
    "# label은 0,1 이므로 변수들만 정규화\n",
    "x_data = df.drop('admit', axis=1, inplace=False).values\n",
    "t_data = df['admit'].values.reshape(-1,1)\n",
    "\n",
    "scaler_x = MinMaxScaler()  # MinMaxScaler 클래스 객체 생성\n",
    "scaler_x.fit(x_data)\n",
    "norm_x_data = scaler_x.transform(x_data)  # (382, 3)\n",
    "\n",
    "# 예측에 쓸 내 성적 => 정규화\n",
    "my_score = np.array([[600, 3.8, 1.]])\n",
    "scaler_x.fit(my_score)\n",
    "norm_my_score = scaler_x.transform(my_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn 결과값\n",
      "예측결과 : 1, 불합격할 확률 : 0.43740782354334207, 합격할 확률 : 0.5625921764566579\n"
     ]
    }
   ],
   "source": [
    "# 1. sklearn 구현\n",
    "\n",
    "# Logistic Regression Model 생성\n",
    "model = linear_model.LogisticRegression()\n",
    "\n",
    "# Traning Data Set을 이용한 학습\n",
    "model.fit(x_data,t_data.ravel())\n",
    "\n",
    "# Prediction\n",
    "my_score = np.array([[600, 3.8, 1.]])\n",
    "pred_val = model.predict(my_score)\n",
    "pred_proba = model.predict_proba(my_score)\n",
    "print('sklearn 결과값')\n",
    "print('예측결과 : {}, 불합격할 확률 : {}, 합격할 확률 : {}'.format(pred_val[0],pred_proba[0,0],pred_proba[0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python 결과값\n",
      "예측결과 : 0, 합격할 확률 : 0.22215945369769727\n"
     ]
    }
   ],
   "source": [
    "# 2. python 구현\n",
    "\n",
    "# Weight & bias\n",
    "W = np.random.rand(3,1)\n",
    "b = np.random.rand(1)\n",
    "\n",
    "# Loss Function\n",
    "def loss_func(input_obj):\n",
    "    # input_obj : W와 b를 같이 포함하고 있는 ndarray => [W1,W2,W3, ... , b]\n",
    "    num_of_bias = b.shape[0]\n",
    "    input_W = input_obj[:-1 * num_of_bias].reshape(-1,num_of_bias)  # 행렬연산을 하기 위한 W 추출\n",
    "    input_b = input_obj[-1 * num_of_bias]\n",
    "    \n",
    "    # 우리 모델의 예측값 : Linear Regression model(Wx + b) ==> sigmoid 적용\n",
    "    z = np.dot(norm_x_data,input_W) + input_b\n",
    "    y = 1 / (1 + np.exp(-1*z))\n",
    "    \n",
    "    delta = 1e-7 #  0에 가까운 작은 값을 줌으로써 프로그램의 로그 연산시 무한대로 발산하는 것을 방지\n",
    "    \n",
    "    # Cross Entropy\n",
    "    return -np.sum(t_data * np.log(y+delta) + ((1-t_data) * np.log(1-y+delta)))\n",
    "    \n",
    "# Learning rate\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# 학습\n",
    "for step in range(30000):\n",
    "    \n",
    "    input_param = np.concatenate((W.ravel(), b.ravel()), axis = 0)\n",
    "    derivative_result = learning_rate * numerical_derivative(loss_func, input_param)\n",
    "    \n",
    "    \n",
    "    num_of_bias = b.shape[0]\n",
    "    \n",
    "    W = W - derivative_result[:-1 * num_of_bias].reshape(-1,num_of_bias)\n",
    "    b = b - derivative_result[-1 * num_of_bias]\n",
    "    \n",
    "        \n",
    "# Prediction => W,b를 구해서 Logistic Regression Model을 완성\n",
    "def logistic_predict(x):\n",
    "    \n",
    "    z = np.dot(x,W) + b\n",
    "    y = 1 / (1 + np.exp(-1 * z))\n",
    "    \n",
    "    if y < 0.5 :\n",
    "        result = 0\n",
    "    else :\n",
    "        result = 1\n",
    "    \n",
    "    return result, y\n",
    "\n",
    "result = logistic_predict(norm_my_score)\n",
    "print('python 결과값')\n",
    "print('예측결과 : {}, 합격할 확률 : {}'.format(result[0],result[1][0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W : [[ 1.1061417]\n",
      " [-1.4273796]\n",
      " [-1.0558444]], b : [-0.49062976], loss : 0.6492480635643005\n",
      "W : [[ 1.1149737 ]\n",
      " [ 0.99957657]\n",
      " [-1.6384919 ]], b : [-1.1837913], loss : 0.5792050361633301\n",
      "W : [[ 1.0780456]\n",
      " [ 1.1205627]\n",
      " [-1.6168962]], b : [-1.2483236], loss : 0.5790991187095642\n",
      "W : [[ 1.0754619]\n",
      " [ 1.1288074]\n",
      " [-1.6152211]], b : [-1.2527866], loss : 0.5790985822677612\n",
      "W : [[ 1.0753133]\n",
      " [ 1.1293391]\n",
      " [-1.6151268]], b : [-1.2530779], loss : 0.5790985822677612\n",
      "W : [[ 1.0753133]\n",
      " [ 1.1293391]\n",
      " [-1.6151268]], b : [-1.2530779], loss : 0.5790985822677612\n",
      "W : [[ 1.0753133]\n",
      " [ 1.1293391]\n",
      " [-1.6151268]], b : [-1.2530779], loss : 0.5790985822677612\n",
      "W : [[ 1.0753133]\n",
      " [ 1.1293391]\n",
      " [-1.6151268]], b : [-1.2530779], loss : 0.5790985822677612\n",
      "W : [[ 1.0753133]\n",
      " [ 1.1293391]\n",
      " [-1.6151268]], b : [-1.2530779], loss : 0.5790985822677612\n",
      "W : [[ 1.0753133]\n",
      " [ 1.1293391]\n",
      " [-1.6151268]], b : [-1.2530779], loss : 0.5790985822677612\n",
      "tensorflow 결과값\n",
      "합격할 확률 : [[0.22216779]]\n"
     ]
    }
   ],
   "source": [
    "# 3. tensorflow 구현\n",
    "\n",
    "# Placeholder 생성\n",
    "X = tf.placeholder(shape=[None,3], dtype=tf.float32)  # 독립변수가 1개인 경우(simple), shape 명시 x\n",
    "T = tf.placeholder(shape=[None,1], dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "W = tf.Variable(tf.random.normal([3,1]), name='weight')\n",
    "b = tf.Variable(tf.random.normal([1]), name='weight')\n",
    "\n",
    "# Hypothesis\n",
    "logit = tf.matmul(X,W) + b  # Linear Regression Hypothesis\n",
    "H = tf.sigmoid(logit)\n",
    "\n",
    "# Loss Function\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit, labels=T))\n",
    "\n",
    "# Train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-1).minimize(loss)\n",
    "\n",
    "# Session & 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "for step in range(30000):\n",
    "    _, W_val, b_val, loss_val = sess.run([train,W,b,loss], feed_dict={X : norm_x_data, T : t_data})\n",
    "    \n",
    "    if step % 3000 == 0 :\n",
    "        print('W : {}, b : {}, loss : {}'.format(W_val, b_val, loss_val))\n",
    "    \n",
    "# Prediction\n",
    "result = sess.run(H, feed_dict={X : norm_my_score})\n",
    "print('tensorflow 결과값')\n",
    "print('합격할 확률 : {}'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data_env] *",
   "language": "python",
   "name": "conda-env-data_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
