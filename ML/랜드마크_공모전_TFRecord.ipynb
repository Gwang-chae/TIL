{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 라이브러리 불러오기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Virtual devices cannot be modified after being initialized\n"
     ]
    }
   ],
   "source": [
    "# 데이터 처리 라이브러리\n",
    "import os\n",
    "import os.path as pth\n",
    "from tqdm import tqdm\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Tensorflow 관련 라이브러리\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential,Model, load_model\n",
    "from tensorflow.keras.layers import Conv2D, Dense, MaxPooling2D, Dropout, BatchNormalization, Flatten, Activation, Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# GPU 설정\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # 특정 GPU에 1GB 메모리만 할당하도록 제한\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[1], 'GPU')\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[1],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=3000)])\n",
    "    except RuntimeError as e:\n",
    "    # 프로그램 시작시에 가상 장치가 설정되어야만 합니다\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 데이터 불러오기 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-1. Labeling 데이터프레임 불러오기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터 준비 \n",
    "path = \"./data/landmark\"  \n",
    "label_df = pd.read_csv(path + '/category.csv') # 각 랜드마크별 label\n",
    "label_dict = dict(label_df[['landmark_name', 'landmark_id']].values)\n",
    "label_dict_reverse = dict(label_df.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-2. Train Data 경로 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "310"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Train 파일(JPG)명과 label 정보를 담은 데이터 프레임 생성\n",
    "train_dirs = path + '/train'\n",
    "files = []\n",
    "categories=[]\n",
    "for img_dir in os.listdir(train_dirs):\n",
    "    img_dir_list = train_dirs + '/' + img_dir\n",
    "    \n",
    "    for filename in os.listdir(img_dir_list):\n",
    "        file_dir = img_dir + '/' + filename\n",
    "        files.append(file_dir)\n",
    "        categories.append(label_dict[img_dir])\n",
    "            \n",
    "train_data=pd.DataFrame(\n",
    "                    {\"file\":files,\n",
    "                    \"label\":categories}\n",
    "                )    \n",
    "\n",
    "label_dict[img_dir]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-3. Test Data 경로 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./data/landmark/test/0/0hmnf5orki.JPG</td>\n",
       "      <td>0hmnf5orki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./data/landmark/test/0/0bgj9co0zl.JPG</td>\n",
       "      <td>0bgj9co0zl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./data/landmark/test/0/03123sl42g.JPG</td>\n",
       "      <td>03123sl42g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./data/landmark/test/0/0vwaki2su2.JPG</td>\n",
       "      <td>0vwaki2su2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./data/landmark/test/0/09jgq862fk.JPG</td>\n",
       "      <td>09jgq862fk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37959</th>\n",
       "      <td>./data/landmark/test/h/hf700dgjt3.JPG</td>\n",
       "      <td>hf700dgjt3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37960</th>\n",
       "      <td>./data/landmark/test/h/hf5vrn6vdx.JPG</td>\n",
       "      <td>hf5vrn6vdx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37961</th>\n",
       "      <td>./data/landmark/test/h/hr5xdtptl2.JPG</td>\n",
       "      <td>hr5xdtptl2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37962</th>\n",
       "      <td>./data/landmark/test/h/hj455fxmjr.JPG</td>\n",
       "      <td>hj455fxmjr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37963</th>\n",
       "      <td>./data/landmark/test/h/hypt0hacjl.JPG</td>\n",
       "      <td>hypt0hacjl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37964 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        file          id\n",
       "0      ./data/landmark/test/0/0hmnf5orki.JPG  0hmnf5orki\n",
       "1      ./data/landmark/test/0/0bgj9co0zl.JPG  0bgj9co0zl\n",
       "2      ./data/landmark/test/0/03123sl42g.JPG  03123sl42g\n",
       "3      ./data/landmark/test/0/0vwaki2su2.JPG  0vwaki2su2\n",
       "4      ./data/landmark/test/0/09jgq862fk.JPG  09jgq862fk\n",
       "...                                      ...         ...\n",
       "37959  ./data/landmark/test/h/hf700dgjt3.JPG  hf700dgjt3\n",
       "37960  ./data/landmark/test/h/hf5vrn6vdx.JPG  hf5vrn6vdx\n",
       "37961  ./data/landmark/test/h/hr5xdtptl2.JPG  hr5xdtptl2\n",
       "37962  ./data/landmark/test/h/hj455fxmjr.JPG  hj455fxmjr\n",
       "37963  ./data/landmark/test/h/hypt0hacjl.JPG  hypt0hacjl\n",
       "\n",
       "[37964 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test 데이터 안 '6' 폴더에 있는 체크포인트 오류 데이터 제거 후 실행\n",
    "test_dirs = path + '/test'\n",
    "files = []\n",
    "ids=[]\n",
    "for img_cat in os.listdir(test_dirs):\n",
    "    id_dir = test_dirs + '/' + img_cat\n",
    "    for filename in os.listdir(id_dir):\n",
    "        files.append(id_dir + '/' +filename)\n",
    "        ids.append(filename.split('.JPG')[0])\n",
    "                           \n",
    "test_data = pd.DataFrame(\n",
    "                    {\"file\":files,\n",
    "                    \"id\":ids}\n",
    "                )    \n",
    "\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. TFRecord 데이터 처리 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-1. TFRecord 압축파일 생성 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _floatarray_feature(array):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=array))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "def _validate_text(text):\n",
    "    \"\"\"If text is not str or unicode, then try to convert it to str.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        return text\n",
    "    elif isinstance(text, 'unicode'):\n",
    "        return text.encode('utf8', 'ignore')\n",
    "    else:\n",
    "        return str(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tfrecords(id_list, randmark_id_list, tfrecords_name):\n",
    "    print(\"Start converting\")\n",
    "    options = tf.io.TFRecordOptions(compression_type = 'GZIP')\n",
    "    with tf.io.TFRecordWriter(path=pth.join(tfrecords_name+'.tfrecords'), options=options) as writer:\n",
    "        for id_, randmark_id in tqdm(zip(id_list, randmark_id_list), total=len(id_list), position=0, leave=True):\n",
    "            image_path = pth.join(train_dirs, id_)\n",
    "            _binary_image = tf.io.read_file(image_path)\n",
    "\n",
    "            string_set = tf.train.Example(features=tf.train.Features(feature={\n",
    "                'image_raw': _bytes_feature(_binary_image),\n",
    "                'randmark_id': _int64_feature(randmark_id),\n",
    "                'id': _bytes_feature(id_.encode()),\n",
    "            }))\n",
    "\n",
    "            writer.write(string_set.SerializeToString())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/device:GPU:1'):\n",
    "    train_ids, val_ids, train_landmark_ids, val_landmark_ids = train_test_split(train_data['file'], train_data['label'], test_size=0.2, random_state=42, shuffle=True,\n",
    "                                                                                stratify=train_data['label'])\n",
    "\n",
    "    to_tfrecords(train_ids, train_landmark_ids, pth.join(path, 'tf_record_train'))\n",
    "    to_tfrecords(val_ids, val_landmark_ids, pth.join(path, 'tf_record_valid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_test_tfrecords(id_list, test_id_list, tfrecords_name):\n",
    "    print(\"Start converting\")\n",
    "    options = tf.io.TFRecordOptions(compression_type = 'GZIP')\n",
    "    with tf.io.TFRecordWriter(path=pth.join(tfrecords_name+'.tfrecords'), options=options) as writer:\n",
    "        for id_, test_id in tqdm(zip(id_list, test_id_list), total=len(id_list), position=0, leave=True):\n",
    "            image_path = id_\n",
    "            _binary_image = tf.io.read_file(image_path)\n",
    "\n",
    "            string_set = tf.train.Example(features=tf.train.Features(feature={\n",
    "                'image_raw': _bytes_feature(_binary_image),\n",
    "                'id': _bytes_feature(test_id.encode()),\n",
    "            }))\n",
    "\n",
    "            writer.write(string_set.SerializeToString())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/device:GPU:1'):\n",
    "    test_ids = test_data['id']\n",
    "    to_test_tfrecords(test_data['file'],test_data['id'], pth.join(path, 'tf_record_test'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-2. TFrecord 파일 TEST(불러오기, 시각화)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfrecord_path = pth.join(path, 'tf_record_train.tfrecords')\n",
    "valid_tfrecord_path = pth.join(path, 'tf_record_valid.tfrecords')\n",
    "\n",
    "BUFFER_SIZE = 128\n",
    "BATCH_SIZE = 16\n",
    "NUM_CLASS = 1049"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/device:GPU:1'):\n",
    "    image_feature_description = {\n",
    "        'image_raw': tf.io.FixedLenFeature([], tf.string),\n",
    "        'randmark_id': tf.io.FixedLenFeature([], tf.int64),\n",
    "        # 'id': tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "\n",
    "\n",
    "    \n",
    "def _parse_image_function(example_proto):\n",
    "    return tf.io.parse_single_example(example_proto, image_feature_description)\n",
    "\n",
    "def map_func(target_record):\n",
    "    img = target_record['image_raw']\n",
    "    label = target_record['randmark_id']\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.dtypes.cast(img, tf.float32)\n",
    "    return img, label\n",
    "\n",
    "def prep_func(image, label):\n",
    "    result_image = image / 255\n",
    "    result_image = tf.image.resize(result_image, (224,224))\n",
    "    onehot_label = tf.one_hot(label, depth=NUM_CLASS)\n",
    "    return result_image, onehot_label\n",
    "\n",
    "with tf.device('/device:GPU:1'):\n",
    "    dataset = tf.data.TFRecordDataset(train_tfrecord_path, compression_type='GZIP')\n",
    "    dataset = dataset.map(_parse_image_function, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.map(map_func, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.map(prep_func, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    valid_dataset = tf.data.TFRecordDataset(valid_tfrecord_path, compression_type='GZIP')\n",
    "    valid_dataset = valid_dataset.map(_parse_image_function, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    valid_dataset = valid_dataset.map(map_func, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    valid_dataset = valid_dataset.batch(BATCH_SIZE)\n",
    "    valid_dataset = valid_dataset.map(prep_func, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    valid_dataset = valid_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/device:GPU:1'):\n",
    "    model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_x, batch_y in dataset:\n",
    "    print(batch_x.shape, batch_y.shape)\n",
    "\n",
    "    target_class = np.argmax(batch_y[0].numpy())\n",
    "    print(label_dict_reverse[target_class])\n",
    "    plt.figure()\n",
    "    plt.imshow(batch_x[0].numpy())\n",
    "    # plt.title('{}'.format(category_dict[target_class]))\n",
    "    plt.show()\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_x, batch_y in valid_dataset:\n",
    "    print(batch_x.shape, batch_y.shape)\n",
    "\n",
    "    target_class = np.argmax(batch_y[0].numpy())\n",
    "    print(label_dict_reverse[target_class])\n",
    "    plt.figure()\n",
    "    plt.imshow(batch_x[0].numpy())\n",
    "    # plt.title('{}'.format(category_dict[target_class]))\n",
    "    plt.show()\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tfrecord_path = pth.join(path, 'tf_record_test.tfrecords')\n",
    "\n",
    "with tf.device('/device:GPU:1'):\n",
    "    test_image_feature_description = {\n",
    "        'image_raw': tf.io.FixedLenFeature([], tf.string),\n",
    "        'id': tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "    \n",
    "def test_image_function(example_proto):\n",
    "    return tf.io.parse_single_example(example_proto, test_image_feature_description)\n",
    "\n",
    "def test_map_func(target_record):\n",
    "    img = target_record['image_raw']\n",
    "    label = target_record['id']\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.dtypes.cast(img, tf.float32)\n",
    "    return img, label\n",
    "\n",
    "def test_prep_func(image, label):\n",
    "    result_image = image / 255.\n",
    "   \n",
    "    return result_image, label\n",
    "    \n",
    "\n",
    "with tf.device('/device:GPU:1'):    \n",
    "    test_dataset = tf.data.TFRecordDataset(test_tfrecord_path, compression_type='GZIP')\n",
    "    test_dataset = test_dataset.map(test_image_function, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    test_dataset = test_dataset.map(test_map_func, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    test_dataset = test_dataset.shuffle(BUFFER_SIZE)\n",
    "    test_dataset = test_dataset.batch(BATCH_SIZE)\n",
    "    test_dataset = test_dataset.map(test_prep_func, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    test_dataset = test_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/device:GPU:1'):\n",
    "    for batch_x, batch_y in test_dataset:\n",
    "        print(batch_x.shape, batch_y.shape)\n",
    "        print(batch_y)\n",
    "        plt.figure()\n",
    "        plt.imshow(batch_x[0].numpy())\n",
    "        # plt.title('{}'.format(category_dict[target_class]))\n",
    "        plt.show()\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 모델링 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-1. Base CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/device:GPU:1'):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(8, (3, 3), activation='relu', input_shape=(270, 480, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(16, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "#     model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1049, activation='softmax')) \n",
    "\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/device:GPU:1'):\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/device:GPU:1'):\n",
    "    history = model.fit(dataset,\n",
    "            epochs=50,  \n",
    "            validation_data=valid_dataset,\n",
    "\n",
    "    #     callbacks = callbacks,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset shapes: ((None, 224, 224, 3), (None, 1049)), types: (tf.float32, tf.float32)>\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization, Conv2D, Activation , AveragePooling2D , Input ,Dropout\n",
    "from tensorflow.keras.layers import Dense,  MaxPooling2D, Add, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras import models, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset shapes: ((None, 224, 224, 3), (None, 1049)), types: (tf.float32, tf.float32)>\n",
      "<PrefetchDataset shapes: ((None, 224, 224, 3), (None, 1049)), types: (tf.float32, tf.float32)>\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "print(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "efficientnetb0 (Functional)  (None, 1000)              5330571   \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 128)               128128    \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1049)              135321    \n",
      "=================================================================\n",
      "Total params: 5,594,532\n",
      "Trainable params: 263,705\n",
      "Non-trainable params: 5,330,827\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 20 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'logs' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-5664c9faaabf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     )\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1102\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m         \u001b[0mepoch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m         \u001b[0;31m# Run validation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'logs' referenced before assignment"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.applications.efficientnet import *\n",
    "\n",
    "efficientnet = EfficientNetB0(weights ='imagenet', include_top = True, \n",
    "                              input_shape = (224, 224, 3), classifier_activation='softmax')\n",
    "\n",
    "earlystop = EarlyStopping(patience=5)\n",
    "learning_rate_reduction=ReduceLROnPlateau(\n",
    "                        monitor= \"val_loss\", \n",
    "                        patience = 3, \n",
    "                        factor = 0.5, \n",
    "                        min_lr=1e-7,\n",
    "                        verbose=1)\n",
    "\n",
    "model_check = ModelCheckpoint( #에포크마다 현재 가중치를 저장    \n",
    "        filepath=\"landmark_efficientnetb0.h5\", #모델 파일 경로\n",
    "        monitor='val_loss',  # val_loss 가 좋아지지 않으면 모델 파일을 덮어쓰지 않음.\n",
    "        save_best_only=True)\n",
    "\n",
    "callback = [earlystop, learning_rate_reduction, model_check]\n",
    "\n",
    "for layer in efficientnet.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "with tf.device('/device:GPU:1'):\n",
    "    model = models.Sequential()\n",
    "    model.add(efficientnet)\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1049, activation='softmax')) \n",
    "    model.summary()\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adam(learning_rate=0.0005),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(dataset,\n",
    "        batch_size=32,\n",
    "        steps_per_epoch = 1,\n",
    "        epochs=20,\n",
    "        validation_data=valid_dataset,\n",
    "        callbacks = callback\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2_latest_p37]",
   "language": "python",
   "name": "conda-env-tensorflow2_latest_p37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
