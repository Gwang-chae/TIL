{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이번에는 Machine Learning에 대해서 알아보아요!!\n",
    "\n",
    "# 먼저 3가지 용어부터 정리하도록 해요!!\n",
    "\n",
    "# AI : (Artificial Intelligence - 인공지능) : 인간이 가지고 있는 \n",
    "# 학습능력, 응용력, 추론능력 컴퓨터를 통해서 구현하고자 하는 가장\n",
    "# 포괄적인 개념\n",
    "\n",
    "# Machine Learning : AI를 구현하는 하나의 방법론. 데이터를 이용해서\n",
    "# 데이터의 특성과 패턴을 학습하고 그 결과를 바탕으로 미지의 데이터에 대한\n",
    "# 미래결과를 예측하는 프로그래밍 기법. \n",
    "# 이런 Machine Learning을 구현하기 위한 방법.\n",
    "# Regression, SVM(Support Vector MAchine), Random Forest, Descision Tree,\n",
    "# Neural Network(신경망), Clustering, Reinforcement Learning, 등등등..\n",
    "# Data Mining : 데이터간의 상관관계나 새로운 속성(feature)을 찾는것이\n",
    "# 주 목적인 작업\n",
    "\n",
    "# Deep Learning : Machine Learning의 한 부분. Nerural Network을 이용해서 학습하는\n",
    "# 알고리즘의 집합.(CNN, RNN, LSTM, GAN, ...)\n",
    "\n",
    "# AI > Machine Learning > Deep Learning\n",
    "\n",
    "#################################################\n",
    "\n",
    "# Machine Learning이 왜 필요할까요??\n",
    "# Machine Learning이라는 개념은 1960년대 개념이 만들어졌어요!\n",
    "\n",
    "# Machine Learning은 Explicit program의 한계때문에 고안되었어요!\n",
    "# Explicit program은 rule based program이라고 해요!\n",
    "# 이런 Explicit program을 이용하면 왠만한 프로그램은 싹다 구현이\n",
    "# 가능해요!!\n",
    "# Explicit programming으로 할 수 없는 프로그램들이 있어요!!\n",
    "# rule이 너무 많아요!! 조건이 너무 많아서..프로그램으로 표현하기\n",
    "# 힘든거예요!\n",
    "# 바로바로대출   => 대출이라는 글자를 찾아서 메일을 spam\n",
    "# 바로바로대~출  => 대~출이라는 글자를 찾아서 메일을 spam\n",
    "# 바로바로대^^출 => \n",
    "\n",
    "# 자율주행시스템 => 너무 조건을 생각해서 차량을 운행해야 하기 \n",
    "# 때문에\n",
    "# 바둑\n",
    "\n",
    "# Explicit 프로그램의 한계때문에 Machine LEarning개념이 도입\n",
    "# 1960년대에 도입!\n",
    "# MAchine Learning : 프로그램 자체가 데이터를 기반으로 학습을\n",
    "# 통해 배우는 능력을 가지는 프로그램을 지칭!\n",
    "\n",
    "##################################################\n",
    "\n",
    "# Machine Learning의 Type\n",
    "\n",
    "# Machine Learning은 학습방법에 따라서 크게 4가지로 분류!!\n",
    "# - 지도학습(Supervised Learning)\n",
    "# - 비지도학습(Unsupervised Learning)\n",
    "# - 준지도학습(SemiSupervised LEarning)\n",
    "# - 강화학습(Reinforcement Learning)\n",
    "\n",
    "# 이 4개중에 우리가 관심이 있어하는 학습방법은 \n",
    "# 지도학습(Supervised Learning)\n",
    "# 우리가 해결해야 하는 현실세계의 대부분의 문제가 지도학습문제\n",
    "\n",
    "# - 지도학습(Supervised Learning)\n",
    "# 지도학습은 학습에 사용되는 데이터와 그 정답(label)을 이용해서\n",
    "# 데이터의 특성과 분포를 학습하고 미래결과를 예측하는 방법.\n",
    "\n",
    "# 지도학습은 어떤 종류의 미래값을 예측하느냐에 따라\n",
    "# Regression(회귀)\n",
    "# Classification(분류) - binary classification\n",
    "#                      - multinomial classification \n",
    "\n",
    "# 비지도학습(unsupervised learning)\n",
    "# 비지도학습은 학습에 사용되는 데이터가.. label이 없는 데이터가\n",
    "# 사용되요!! 이 부분이 지도학습과 가장 큰 차이!!\n",
    "# 비지도학습은 정답(label)이 없는 데이터만을 이용하기 때문에\n",
    "# 입력값 자체가 가지고 있는 특성과 분포를 이용해서 Grouping하는\n",
    "# Clustering(군집화)하는데 주로 사용!!\n",
    "\n",
    "# 준지도학습 = 지도학습 + 비지도학습\n",
    "# 데이터의 일부분만 label이 제공되는 경우!!\n",
    "\n",
    "\n",
    "# 강화학습 = 위에서 말한 3가지 방식과는 완전히 다른 학습 방법\n",
    "# Agent, Environment, Action, Reward 개념을 이용\n",
    "# 게임쪽에서 많이 사용되는 학습방법 => 바둑.\n",
    "# Google 알파고의 메인 알고리즘이 바로 이 강화학습.\n",
    "\n",
    "###################################################\n",
    "\n",
    "# Machine Learning 학습방법 중 \n",
    "# Supervised Learning(지도학습)\n",
    "# 지도학습은 입력값(x)와 Label이라고 표현되는 정답(t)를 포함하는\n",
    "# Training Data Set을 이용해서 학습을 진행!\n",
    "\n",
    "# 학습된 결과를 바탕으로 => Predictive model을 만들어요! =>\n",
    "# 미지의 데이터에 대해 미래값을 예측하는 작업을 진행!\n",
    "\n",
    "# 공부시간에 따른 시험점수 예측     (점수)\n",
    "# 공부시간에 따른 시험합격여부 예측 (합격/불합격)\n",
    "# 카드사용패턴에 따른 도난신용카드 판별\n",
    "# 공부시간에 따른 시험Grade예측 (A/B/C/D/F)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI(인공지능 - Artificial Intelligence)\n",
    "# 사람이 가지는 가장 큰 특성이라고 할 수 있는 학습능력, 추론능력을\n",
    "# 컴퓨터를 통해서 구현하는 가장 큰 포괄적인 개념\n",
    "\n",
    "# Machine Learning : AI를 구현하기 위한 하나의 방법\n",
    "# 데이터를 이용해서 데이터의 특성과 패턴을 파악한 해당 내용을 학습해서\n",
    "# 그 결과를 바탕으로 미지의 데이터에 대해 미래 결과를 예측.\n",
    "\n",
    "# 1. Regression(Linear Regression, Logistice Regression)\n",
    "# 2. SVM(Support Vector Machine)\n",
    "# 3. Decision Tree\n",
    "# 4. Random Forest\n",
    "# 5. Naive Bayes\n",
    "# 6. KNN(K-Nearest Neighbor)\n",
    "# 7. Neural Network(신경망)\n",
    "# 8. Clustering(K-Means, DBSCAN)\n",
    "# 9. Reinforcement Learning\n",
    "\n",
    "# Deep Learning : Machine Learning을 구현하기 위한 하나의 방법인 신경망을\n",
    "#                 이용하는 구조 + 알고리즘의 집합\n",
    "#                 CNN, RNN, LSTN, GAN, ...\n",
    "\n",
    "# Machine Learning이 왜 필요할까?\n",
    "# Explicit program의 한계\n",
    "# Explicit program => Rule-based programming\n",
    "# 조건이 많은 경우 한계에 봉착\n",
    "# 프로그램 자체가 데이터를 기반으로 학습을 통해 스스로 배우는 능력을 가질 수 \n",
    "# 있도록 프로그래밍하는 기법\n",
    "\n",
    "# ====================================================================\n",
    "\n",
    "# 1. 지도학습(Supervised Learning)\n",
    "#    우리가 현실에서 해결해야 하는 대부분의 문제\n",
    "# 2. 비지도학습(Unsupervised Learning)\n",
    "# 3. 준지도학습(Semisupervised Learning)\n",
    "# 4. 강화학습(Reinforcement Learning)\n",
    "\n",
    "\n",
    "# 1. 지도학습(Supervised Learning)\n",
    "# 입력값(X)과 Label(정답, t)을 포함하는 Training Data Set을 이용하여 학습을 진행\n",
    "# 그 학습된 결과를 바탕으로 미지의 데이터에 대한 미래 예측값을 알아내는 방법\n",
    "# - 공부시간에 따른 시험점수 예측       continuous value(연속적인 값)\n",
    "# - 공부시간에 따른 시험합격 여부 예측  value(이산값) 0(False) or 1(True)\n",
    "# - 공부시간에 따른 시험 Grade 예측     value(이산값) 여러개 중 1개\n",
    "\n",
    "# 지도학습은 어떤 종류의 미래값을 예측하느냐에 따라\n",
    "# Regression(회귀), Classification(분류)\n",
    "# Regression(회귀) : 학습된 결과를 바탕으로 연속적인 숫자값을 예측\n",
    "# Linear Regression(선형회귀) => '얼마나'인지를 예측하는 방법\n",
    "\n",
    "# 하지만 Logistic Regression은 분류기법\n",
    "# Classification(분류) : 학습된 결과를 바탕으로 주어진 입력값이 \n",
    "#                        어떤 종류의 값(discrete value)인지를 예측\n",
    "#                        '어떤것'인지를 예측\n",
    "#                         binary classification(둘 중 하나를 예측)\n",
    "#                         multinomial classification(여러개 중 하나를 예측)\n",
    "\n",
    "# 2. 비지도학습(Unsupervised Learning)\n",
    "# 입력되는 데이터에 정답(label)이 포함되어 있지 않음\n",
    "# 데이터의 분포, 특징을 학습해서 Grouping\n",
    "# Clustering(군집화)\n",
    "\n",
    "# Classification(분류), Clustering(군집화)\n",
    "# Classification(분류) : 비슷한 데이터끼리 Grouping\n",
    "# Clustering(군집화) : 어떤 분류에 속하는 가에 대한 정답을 예측\n",
    "\n",
    "# 3. 준지도학습(Semisupervised Learning)\n",
    "# 대표적인 예 => Google Photos\n",
    "\n",
    "# 4. 강화학습(Reinforcement Learning)\n",
    "# 추후 개념정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수치미분\n",
    "\n",
    "# 미분\n",
    "# 어떤 함수의 정의역 속 각 점에서\n",
    "# 독립변수의 값의 변화량과 함수값의 변화량 비율의 극한 혹은 극한의 집합을\n",
    "# 치역으로 가지는 새로운 함수\n",
    "\n",
    "# 이런 새로운 함수를 구할 수 있는데 이를 미분이라 하고 derivative라고 함\n",
    "# 약간 다른 의미로 이런 미분함수(도함수)를 구하는 작업을 미분법이라 하고 differntiation이라 함\n",
    "\n",
    "# 미분은 함수에 있어서 특정 순간의 변화량을 의미\n",
    "# x값에서의 작은 변화가 함수 f(x)를 얼마나 변화시키는가?\n",
    "\n",
    "# 미분은 크게 2가지 방식\n",
    "# 종이와 펜을 가지고 논리적 전개로 미분을 수행하는 해석미분\n",
    "# (Analytical differentiation)\n",
    "\n",
    "# 해석미분을 이용해서 문제를 해결할 수 없을 때 수치적 근사값을 이용해서 미분 수행\n",
    "# 이 방법을 수치미분!\n",
    "# 한가지 조심할 점으로 delta_x라고 0과 근사한 값을 이용해야 함\n",
    "# 이 delta_x를 소수점 8자리 이하로 낮추면 안됨(파이썬) => 8자리 이하로 낮추면 반올림 연산으로 오차 발생\n",
    "# 일반적으로 delta_x를 0.00001수준으로 설정해서 수치미분을 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 일변수 함수의 수치미분 코드를 작성\n",
    "\n",
    "# 입력으로 들어오는 x에서 아주 미세하게 값이 변화할 때\n",
    "# 함수 f는 얼마나 변하는지를 수치적으로 계산해서 return\n",
    "\n",
    "def numerical_derivative(f,x):\n",
    "    \n",
    "#     f : 미분하려는 함수\n",
    "#         Python은 일급함수를 지원\n",
    "#         하나의 함수를 다른 함수의 인자로 전달 가능\n",
    "#         일반적으로 외부 def, lambda를 이용해서 정의\n",
    "#     x : 미분값을 알고자 하는 입력값\n",
    "        \n",
    "#     delta_x는 0과 가까운 작은값을 이용하고 1e-8이하는 사용하면 안됨\n",
    "#     컴퓨터의 반올림 오차때문에 엉뚱한 값 등장\n",
    "#     delta_x는 1e-4 ~ 1e-6정도의 수로 설정 => 일반적으로 1e-4\n",
    "    delta_x = 1e-4\n",
    "    \n",
    "#     중앙차분으로 미분 수행\n",
    "    return (f(x+delta_x) - f(x-delta_x)) / (2 * delta_x)\n",
    "\n",
    "# 미분하려는 함수 => f(x) = x^2\n",
    "def my_func(x):\n",
    "    \n",
    "    return x**2\n",
    "\n",
    "# 함수 f(x)에서 미분계수 f'(5)값 구하기\n",
    "\n",
    "result = numerical_derivative(my_func,5)\n",
    "print('result : {}'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제로 우리가 머신러닝, 딥러닝에서 사용할\n",
    "# 미분코드를 작성\n",
    "\n",
    "# 다변수함수에 대한 미분\n",
    "# 다변수함수인 경우 입력변수가 하나 이상이므로\n",
    "# 이 입력변수들은 서로 독립이기 때문에\n",
    "# 수치미분 할 때 편미분해서 개별적으로 계산\n",
    "\n",
    "# f(x,y) = 2x + 3xy + y^3\n",
    "# f'(1.0,2.0) = (8,15)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def numerical_derivative(f,x):\n",
    "    \n",
    "#     f : 미분하려 하는 다변량함수\n",
    "#     x : 모든 변수를 포함하고 있어야 함! ndarray\n",
    "    delta_x = 1e-4\n",
    "    derivative_x = np.zeros_like(x)  # 미분한 결과를 저장할 ndarray\n",
    "    \n",
    "#     iterator를 이용해서 입력변수 x에 대해 편미분을 수행\n",
    "    it = np.nditer(x, flags=['multi_index'])\n",
    "    \n",
    "    while not it.finished:\n",
    "        idx = it.multi_index  # iterator의 현재 index 호출\n",
    "        \n",
    "        # 현재 칸의 값을 잠시 저장\n",
    "        tmp = x[idx]\n",
    "        \n",
    "        x[idx] = tmp + delta_x\n",
    "        fx_plus_delta = f(x)  # f(x + delta_x)\n",
    "        \n",
    "        x[idx] = tmp - delta_x\n",
    "        fx_minus_delta = f(x) # f(x - delta_x)\n",
    "        \n",
    "        derivative_x[idx] = (fx_plus_delta - fx_minus_delta) / (2 * delta_x)\n",
    "        \n",
    "        x[idx] = tmp  # 현재 데이터를 원상 복구 => 다음 편미분을 위해\n",
    "        it.iternext()\n",
    "        \n",
    "    return derivative_x\n",
    "\n",
    "# def my_func(x):\n",
    "    \n",
    "#     return x**2\n",
    "\n",
    "# # 함수 f(x) = x^2 에서 미분계수 f'(3)값 구하기\n",
    "\n",
    "# result = numerical_derivative(my_func,np.array([3.0]))\n",
    "# print('result : {}'.format(result))\n",
    "\n",
    "# f(x,y) = 2x + 3xy + np.power(y,3)\n",
    "def my_func(input_data):\n",
    "    x = input_data[0]\n",
    "    y = input_data[1]\n",
    "    \n",
    "    return 2*x + 3*x*y + np.power(y,3)\n",
    "\n",
    "result = numerical_derivative(my_func, np.array([1.0,2.0]))\n",
    "print('result : {}'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning에서의 Regression\n",
    "# 지도학습은 학습된 결과를 바탕으로 \n",
    "# 미래의 무엇을 예측하느냐에 따라 크게 2가지로 구분!\n",
    "\n",
    "# Regression\n",
    "# - Linear Regression\n",
    "#   => Training Data Set을 이용해서 학습된 결과를 만들고\n",
    "#      연속적인 숫자값을 예측하는 것을 의미해요!\n",
    "# - Logistic Regression\n",
    "\n",
    "# Classification\n",
    "# - binary classification\n",
    "# - multinomial classification\n",
    "\n",
    "# 공부시간(x), 시험점수(t)를 이용해서 데이터를 표현\n",
    "# 이 데이터를 2차원 평면에 scatter(산점도)를 이용해서 표현\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = { '공부시간(x)' : [1,2,3,4,5,7,8,10,12,13,14,15,18,20,25,28,30],\n",
    "         '시험점수(t)' : [5,7,20,31,40,44,46,49,60,62,70,80,85,91,92,97,98]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "display(df.head())\n",
    "\n",
    "plt.scatter(df['공부시간(x)'],df['시험점수(t)'])  # 산점도\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 데이터를 이용해서 \n",
    "# 이 데이터를 가장 잘 표현하는 직선을 하나 작성!!\n",
    "# 직선이니까..y = ax + b (a는 기울기(slop), b는 절편(intercept))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = { '공부시간(x)' : [1,2,3,4,5,7,8,10,12,13,14,15,18,20,25,28,30],\n",
    "         '시험점수(t)' : [5,7,20,31,40,44,46,49,60,62,70,80,85,91,92,97,98]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "plt.scatter(df['공부시간(x)'],df['시험점수(t)'])  # 산점도를 구할 수 있어요!\n",
    "# 직선을 표현하는게...y = ax + b\n",
    "plt.plot(df['공부시간(x)'],df['공부시간(x)']*2 + 3, color='g')\n",
    "plt.plot(df['공부시간(x)'],df['공부시간(x)']*5 - 7, color='r')\n",
    "plt.plot(df['공부시간(x)'],df['공부시간(x)']*1 + 8, color='b')\n",
    "plt.plot(df['공부시간(x)'],df['공부시간(x)']*4 - 10, color='magenta')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 초기의 기울기 a와 절편 b는 랜덤하게 결정되고 그 이 후 우리가 가지는 데이터의 \n",
    "# 특성을 가장 잘 표현하기 위한 직선을 찾아가는 과정이 진행되게 됩니다.!!\n",
    "# 이 과정을 우리는 학습(learning)!!\n",
    "# 결국은 데이터를 가장 잘 표현하는 직선(y)를 만들 수 있고\n",
    "# 이 직선을 predictive model이라고 해요! => linear regression\n",
    "\n",
    "# y = ax + b (수학적 표현)\n",
    "# y = Wx + b (W : weight, 가중치) (b : bias)\n",
    "\n",
    "# 대체 W와 b는 어떻게 결정이 되는것인가??\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오차\n",
    "# 오차를 이해하기 위해 선 1개만을 가지고 생각!\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = { '공부시간(x)' : [1,2,3,4,5,7,8,10,12,13,14,15,18,20,25,28,30],\n",
    "         '시험점수(t)' : [5,7,20,31,40,44,46,49,60,62,70,80,85,91,92,97,98]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "plt.scatter(df['공부시간(x)'],df['시험점수(t)'])  # 산점도를 구할 수 있어요!\n",
    "plt.plot(df['공부시간(x)'],df['공부시간(x)']*5 - 7, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function의 모양을 확인하기 위해 graph를 그려보아요!\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# training data set\n",
    "x = np.arange(1,101)\n",
    "t = np.arange(1,101)\n",
    "\n",
    "# W\n",
    "W = np.arange(-10,13)\n",
    "\n",
    "loss = []\n",
    "\n",
    "for tmp in W:\n",
    "    loss.append(np.power((t-tmp*x),2).mean())\n",
    "\n",
    "plt.plot(W,loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression Model의 이론적인 부분을 살펴보았으니\n",
    "# 해당 내용을 Python으로 구현해 보아요!!\n",
    "\n",
    "# 코드를 구현하는 절차는 다음과 같아요!!\n",
    "\n",
    "# 1. Training Data Set을 준비\n",
    "#    머신러닝에 입력으로 사용될 데이터를 NumPy array(ndarray)형태로 준비!\n",
    "\n",
    "# 2. Linear Regression Model을 정의\n",
    "#    y = Wx + b => model을 프로그램적으로 표현.\n",
    "#    W와 b에 대한 변수 선언한 후 초기값은 랜덤값을 이용할 꺼예요!!\n",
    "\n",
    "# 3. loss function을 정의 : 손실함수(loss function)에 대한 코드를 작성\n",
    "#                           행렬 처리해야 해요!!\n",
    "\n",
    "# 4. learning rate의 정의 : 일반적으로 customizing이 되는 값으로 초기에는\n",
    "#                           0.001정도로 설정해서 사용하고 loss값을 보고\n",
    "#                           수치를 조절할 필요가 있어요!\n",
    "\n",
    "# 5. 학습을 진행 : 반복적으로 편미분을 이용해서 W와 b를 update하는 방식으로\n",
    "#                  구현.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Training Data Set\n",
    "x_data = np.array([1,2,3,4,5,7,8,10,12,13,14,15,18,20,25,28,30]).reshape(-1,1)\n",
    "t_data = np.array([5,7,20,31,40,44,46,49,60,62,70,80,85,91,92,97,98]).reshape(-1,1)\n",
    "\n",
    "# Linear Regression Model을 정의\n",
    "W = np.random.rand(1,1)  # matrix\n",
    "b = np.random.rand(1)    # scalar\n",
    "\n",
    "# Hypothesis는 따로 표현하지 않아요!\n",
    "\n",
    "# loss function\n",
    "def loss_func(x,t):\n",
    "    \n",
    "    y = np.dot(x,W) + b\n",
    "    \n",
    "    return np.mean(np.power((t-y),2))    # 최소제곱법\n",
    "\n",
    "# 미분함수\n",
    "def numerical_derivative(f,x):\n",
    "    # f : 미분하려고 하는 다변수 함수\n",
    "    # x : 모든 변수를 포함하고 있어야 해요! ndarray(차원상관없이)\n",
    "    \n",
    "    delta_x = 1e-4\n",
    "    derivative_x = np.zeros_like(x)  # 미분한 결과를 저장하는 ndarray\n",
    "    \n",
    "    # iterator를 이용해서 입력변수 x에 대해 편미분을 수행\n",
    "    it = np.nditer(x, flags=['multi_index'])\n",
    "    \n",
    "    while not it.finished:\n",
    "        \n",
    "        idx = it.multi_index   # iterator의 현재 index를 추출(tuple로 추출)\n",
    "        \n",
    "        # 현재 칸의 값을 어딘가에 잠시 저장해야해요!\n",
    "        tmp = x[idx]  \n",
    "        \n",
    "        x[idx] = tmp + delta_x\n",
    "        fx_plus_delta = f(x)  # f(x + delta_x)\n",
    "        \n",
    "        x[idx] = tmp - delta_x\n",
    "        fx_minus_delta = f(x) # f(x - delta_x)\n",
    "    \n",
    "        derivative_x[idx] = (fx_plus_delta - fx_minus_delta) / (2 * delta_x)\n",
    "        \n",
    "        x[idx] = tmp  # 데이터를 원상 복구\n",
    "        \n",
    "        it.iternext()\n",
    "    \n",
    "    return derivative_x\n",
    "\n",
    "# 학습 종료 후 임의의 데이터에 대한 예측값을 알아오는 함수\n",
    "# prediction\n",
    "def predict(x):\n",
    "    \n",
    "    return np.dot(x,W) + b   # Hypothesis, Linear Regression Model\n",
    "\n",
    "# learning rate라는 상수가 필요. 정의해야 해요!\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# 미분을 진행할 loss_func에 대한 lambda 함수를 정의\n",
    "f = lambda x : loss_func(x_data,t_data)\n",
    "\n",
    "\n",
    "# 학습을 진행!!\n",
    "# 반복해서 학습을 진행..(W와 b를 update하면서 반복적으로 학습을 진행!)\n",
    "for step in range(90000):\n",
    "    \n",
    "    W = W - learning_rate * numerical_derivative(f,W)  # W의 편미분\n",
    "    b = b - learning_rate * numerical_derivative(f,b)  # b의 편미분\n",
    "    \n",
    "    if step % 9000 == 0:\n",
    "        print('W : {}, b : {}, loss : {}'.format(W,b,loss_func(x_data,t_data)))\n",
    "        \n",
    "\n",
    "# 학습종료 후 예측\n",
    "print(predict(19))\n",
    "\n",
    "# 데이터의 분포를 scatter로 확인\n",
    "plt.scatter(x_data.ravel(),t_data.ravel())\n",
    "plt.plot(x_data.ravel(), np.dot(x_data,W) + b)  # 직선\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from my_library.machine_learning_library import numerical_derivative\n",
    "\n",
    "# 1. Raw Data Loading\n",
    "df = pd.read_csv('./data/ozone.csv')\n",
    "\n",
    "# 2. Data Preprocessing(데이터 전처리)\n",
    "# - 결측치 처리\n",
    "#     - 삭제, 값을 대체\n",
    "# - 이상치 처리\n",
    "#     - 이상치를 검출하고, 변경하는 작업\n",
    "# - 데이터 정규화 작업\n",
    "#     - 학습에 필요한 컬럼을 추출, 새로 생성\n",
    "\n",
    "# 필요한 column(Temp, Ozone)만 추출\n",
    "training_data = df[['Temp','Ozone']]\n",
    "print(training_data.shape)\n",
    "\n",
    "# 결측치 제거\n",
    "training_data = training_data.dropna(how='any')\n",
    "print(training_data.shape)\n",
    "\n",
    "# 3. Training Data Set\n",
    "x_data = training_data['Temp'].values.reshape(-1,1)\n",
    "t_data = training_data['Ozone'].values.reshape(-1,1)\n",
    "\n",
    "# 4. Simple Linear Regression이므로 y = Wx + b\n",
    "#    W와 b를 정의\n",
    "W = np.random.rand(1,1)\n",
    "b = np.random.rand(1)\n",
    "\n",
    "# 5. loss function 정의\n",
    "def loss_func(x,t):\n",
    "    \n",
    "    y = np.dot(x,W) + b\n",
    "    \n",
    "    return np.mean(np.power((t-y),2)) # 최소제곱\n",
    "\n",
    "# 6. 학습 종료 후 사용할 예측 함수 생성\n",
    "def predict(x):\n",
    "    \n",
    "    return np.dot(x,W) + b\n",
    "\n",
    "# 7. 프로그램에서 필요한 변수 정의\n",
    "learning_rate = 1e-4  # customizing value\n",
    "f = lambda x : loss_func(x_data,t_data)\n",
    "\n",
    "# 8. 학습 진행\n",
    "for step in range(30000):\n",
    "    \n",
    "    W -= learning_rate * numerical_derivative(f,W)  # W의 편미분\n",
    "    b -= learning_rate * numerical_derivative(f,b)  # b의 편미분\n",
    "    \n",
    "    if step % 3000 == 0:\n",
    "        print('W : {}, b : {}, loss : {}'.format(W,b,loss_func(x_data,t_data)))\n",
    "        \n",
    "# 9. 그래프로 회귀선 확인\n",
    "plt.scatter(x_data,t_data)\n",
    "plt.plot(x_data,np.dot(x_data,W)+b, color='r')\n",
    "plt.show()\n",
    "\n",
    "# 10. 예측\n",
    "print(predict(62))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit-learn을 이용해서 같은 데이터로 학습시키고 예측\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "\n",
    "# 1. Raw Data Loading\n",
    "df = pd.read_csv('./data/ozone.csv')\n",
    "\n",
    "# 필요한 column(Temp, Ozone)만 추출\n",
    "training_data = df[['Temp','Ozone']]\n",
    "# print(training_data.shape)\n",
    "\n",
    "# 결측치 제거\n",
    "    training_data = training_data.dropna(how='any')\n",
    "# print(training_data.shape)\n",
    "\n",
    "# 3. Training Data Set\n",
    "x_data = training_data['Temp'].values.reshape(-1,1)\n",
    "t_data = training_data['Ozone'].values.reshape(-1,1)\n",
    "\n",
    "# 4. sklearn을 이용해서 linear regression model 객체를 생성\n",
    "model = linear_model.LinearRegression()\n",
    "\n",
    "# 5. Training Data Set을 이용해서 학습 진행\n",
    "model.fit(x_data,t_data)\n",
    "\n",
    "# 6. W와 b값 확인\n",
    "print('W : {}, b : {}'.format(model.coef_, model.intercept_))\n",
    "\n",
    "# 7. 그래프로 확인\n",
    "plt.scatter(x_data, t_data)\n",
    "plt.plot(x_data, np.dot(x_data,model.coef_) + model.intercept_, color='r')\n",
    "plt.show()\n",
    "\n",
    "#. 8. 예측\n",
    "predict_val = model.predict([[62]]) # 온도를 이용해서 오존량 예측\n",
    "print(predict_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 머신러닝에서는 데이터 전처리가 필수\n",
    "# 결측치 처리\n",
    "# 이상치 처리\n",
    "# 데이터의 정규화\n",
    "\n",
    "# 이상치 처리(Outlier)\n",
    "# Z-Score (분산, 표준편차를 이용하는 이상치 검출방식)\n",
    "# Tukey Outlier(4분위값을 이용하는 이상치 검출방식)\n",
    "# 이상치(Outlier)는 속성의 값이 일반적인 값보다 편차가 큰 값을 의미\n",
    "# 데이터 전체 패턴에서 동떨어져 있는 관측치를 지칭\n",
    "# 평균뿐아니라 분산에도 영향을 미치기 때문에\n",
    "# 결국은 데이터 전체의 안정성을 저해\n",
    "# 따라서, 이상치는 반드시 처리해야 하고,\n",
    "# 이것을 검출하고 처리하는데 상당히 많은 시간이 소요됨\n",
    "\n",
    "# 독립변수(온도)에 있는 이상치를 지대점이라 하고\n",
    "# 종속변수(오존량)에 있는 이상치를 Outlier라고 함\n",
    "\n",
    "# Tukey Outlier를 이용해서 처리\n",
    "# boxplot으로 이상치 확인(4분위값 이용)\n",
    "# boxplot을 사용할 때 이상치를 분류하는 기준은 IQR value를 사용\n",
    "# IQR value = 3사분위값 - 1사분위값\n",
    "# (1사분위값 - 1.5 * IQR value) 보다 작은 값을 이상치로 판별\n",
    "# (3사분위값 + 1.5 * IQR value) 보다 큰 값을 이상치로 판별\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = np.array([1,2,3,4,5,6,7,8,9,10,11,12,13,14,22.1])\n",
    "\n",
    "fig = plt.figure()  # 새로운 그림 생성\n",
    "fig_1 = fig.add_subplot(1,2,1)  # 1행 2열의 subplot의 위치가 1번 위치\n",
    "fig_2 = fig.add_subplot(1,2,2)  # 1행 2열의 subplot의 위치가 2번 위치\n",
    "\n",
    "fig_1.set_title('Original Data Boxplot')\n",
    "fig_1.boxplot(data)\n",
    "\n",
    "# numpy로 사분위수를 구하려면 percentile() 함수를 이용\n",
    "print(np.mean(data))\n",
    "print(np.median(data))\n",
    "print(np.percentile(data,25))  # 1사분위\n",
    "print(np.percentile(data,50))  # 2사분위\n",
    "print(np.percentile(data,75))  # 3사분위\n",
    "\n",
    "# 이상치를 검출하려면 IQR value가 필요\n",
    "IQR_val = np.percentile(data,75) - np.percentile(data,25)\n",
    "\n",
    "upper_fense = np.percentile(data,75) + 1.5 * IQR_val\n",
    "lower_fense = np.percentile(data,25) - 1.5 * IQR_val\n",
    "\n",
    "print('upper_fense : {}'.format(upper_fense))\n",
    "print('lower_fense : {}'.format(lower_fense))\n",
    "\n",
    "# boolean indexing을 이용하면 간단하게 이상치 식별 가능\n",
    "print(data[(data > upper_fense) | (data < lower_fense)])\n",
    "\n",
    "result_data = data[(data <= upper_fense) & (data >= lower_fense)]\n",
    "\n",
    "fig_2.set_title('Remove Outlier Boxplot')\n",
    "fig_2.boxplot(result_data)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "data = np.array([1,2,3,4,5,6,7,8,9,10,11,12,13,14,22.1])\n",
    "\n",
    "zscore_threshold = 1.8  # zscore outlier 임계값 (일반적으로 2)\n",
    "\n",
    "outliers = data[np.abs(stats.zscore(data)) > zscore_threshold]\n",
    "print(outliers)\n",
    "\n",
    "print(data[np.isin(data,outliers, invert=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Linear Regression 예제 작성\n",
    "# sklearn을 이용해서 학습시키고 그 결과를 작성한 것과 비교\n",
    "\n",
    "# 데이터의 전처리 미흡으로 결과가 엉망\n",
    "# 1. 결측치 처리\n",
    "# 2. 이상치 처리(Tukey Outlier, Z-Score)\n",
    "# 3. 정규화(Normalization)\n",
    "# 4. 학습에 필요한 feature들을 선별해서 학습이 잘 이루어지도록\n",
    "#    혹은 새로운 feature를 만들어서 학습에 응용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from my_library.machine_learning_library import numerical_derivative\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# Raw Data Loading\n",
    "df = pd.read_csv('./data/ozone.csv')\n",
    "\n",
    "# 결측치 처리\n",
    "training_data = df[['Temp','Ozone']]\n",
    "training_data = training_data.dropna(how='any')\n",
    "print(training_data.shape)\n",
    "\n",
    "# 이상치 처리\n",
    "zscore_threshold = 2.0 # 97.7% 이상, 2.3% 이하인 값들을 이상치로 식별\n",
    "# Outlier 출력\n",
    "# Temp에 대한 이상치 확인\n",
    "outliers = training_data['Temp'][np.abs(stats.zscore(training_data['Temp'])) > zscore_threshold]\n",
    "training_data = training_data.loc[~training_data['Temp'].isin(outliers)]\n",
    "print(training_data.shape)\n",
    "\n",
    "# Ozone에 대한 이상치를 확인하고 제거\n",
    "outliers = training_data['Ozone'][np.abs(stats.zscore(training_data['Ozone'])) > zscore_threshold]\n",
    "training_data = training_data.loc[~training_data['Ozone'].isin(outliers)]\n",
    "print(training_data.shape)\n",
    "\n",
    "# Training Data Set\n",
    "x_data = training_data['Temp'].values.reshape(-1,1)\n",
    "t_data = training_data['Ozone'].values.reshape(-1,1)\n",
    "\n",
    "# Weight & bias\n",
    "W = np.random.rand(1,1)\n",
    "b = np.random.rand(1)\n",
    "\n",
    "# loss function\n",
    "def loss_func(x,t):\n",
    "    \n",
    "    y = np.dot(x,W) + b\n",
    "    \n",
    "    return np.mean(np.power((t-y),2))\n",
    "\n",
    "# learning_rate\n",
    "learning_rate = 1e-5\n",
    "f = lambda x : loss_func(x_data,t_data)\n",
    "\n",
    "# 학습 진행\n",
    "for step in range(30000):\n",
    "    \n",
    "    W -= learning_rate * numerical_derivative(f,W)\n",
    "    b -= learning_rate * numerical_derivative(f,b)\n",
    "    \n",
    "    if step % 3000 == 0 :\n",
    "        print('W : {}, b : {}, loss : {}'.format(W,b,loss_func(x_data,t_data)))\n",
    "        \n",
    "# 예측\n",
    "def predict(x):\n",
    "    \n",
    "    return np.dot(x,W) + b\n",
    "\n",
    "print(predict(62))\n",
    "# 시각화\n",
    "plt.scatter(x_data,t_data)\n",
    "plt.plot(x_data, np.dot(x_data,W)+b, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터가 가진 feature들의 scale이 심하게 차이나는 경우\n",
    "# 이를 조정해줘야 함 => 정규화(Normalization)\n",
    "\n",
    "# 각 feature들에 대해 동일한 scale을 적용할 필요가 있음 (0 ~ 1 사이 값으로)\n",
    "# Min-Max Normalization (정규화)\n",
    "# Standardization - Z-Score Normalization (표준화)\n",
    "\n",
    "# Min-Max Normalization \n",
    "# 데이터를 정규화하는 가장 일반적인 방법\n",
    "# xi - min(x) / max(x) - min(x)의 수식으로 scaling\n",
    "# 모든 feature들에 대해 동일한 척도를 가지고 scaling\n",
    "# 그러나, 이상치 처리를 하지 않으면 max(x) - min(x)가 크게 변하므로\n",
    "# 이상치 처리에 민감한 방법\n",
    "\n",
    "# Standardization(Z-Score Normalization)\n",
    "# 이상치에 상대적으로 덜 민감\n",
    "# 그러나, 동일한 척도로 scaling되지 않음\n",
    "# xi - mean(x) / var(x)의 수식으로 scaling => 음수가 나올 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from my_library.machine_learning_library import numerical_derivative\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Raw Data Loading\n",
    "df = pd.read_csv('./data/ozone.csv')\n",
    "\n",
    "# 결측치 처리\n",
    "training_data = df[['Temp','Ozone']]\n",
    "training_data = training_data.dropna(how='any')\n",
    "print(training_data.shape)\n",
    "\n",
    "# 이상치 처리\n",
    "zscore_threshold = 2.0 # 97.7% 이상, 2.3% 이하인 값들을 이상치로 식별\n",
    "# Outlier 출력\n",
    "# Temp에 대한 이상치 확인\n",
    "outliers = training_data['Temp'][np.abs(stats.zscore(training_data['Temp'])) > zscore_threshold]\n",
    "training_data = training_data.loc[~training_data['Temp'].isin(outliers)]\n",
    "print(training_data.shape)\n",
    "\n",
    "# Ozone에 대한 이상치를 확인하고 제거\n",
    "outliers = training_data['Ozone'][np.abs(stats.zscore(training_data['Ozone'])) > zscore_threshold]\n",
    "training_data = training_data.loc[~training_data['Ozone'].isin(outliers)]\n",
    "print(training_data.shape)\n",
    "\n",
    "# 정규화 처리\n",
    "# 직접구현도 가능하지만 sklearn 사용\n",
    "# 독립변수와 종속변수의 scaler 객체를 각각 생성\n",
    "scaler_x = MinMaxScaler()  # MinMaxScaler 클래스 객체 생성\n",
    "scaler_t = MinMaxScaler()\n",
    "\n",
    "# scaler에 데이터를 fitting\n",
    "scaler_x.fit(training_data['Temp'].values.reshape(-1,1))\n",
    "scaler_t.fit(training_data['Ozone'].values.reshape(-1,1))\n",
    "print(scaler_x.n_samples_seen_)\n",
    "print(scaler_x.data_min_)\n",
    "\n",
    "# scaler로 데이터 변환\n",
    "training_data['Temp'] = scaler_x.transform(training_data['Temp'].values.reshape(-1,1))\n",
    "training_data['Ozone'] = scaler_t.transform(training_data['Ozone'].values.reshape(-1,1))\n",
    "\n",
    "print(training_data['Temp'].values)\n",
    "print(training_data['Ozone'].values)\n",
    "\n",
    "# Training Data Set\n",
    "x_data = training_data['Temp'].values.reshape(-1,1)\n",
    "t_data = training_data['Ozone'].values.reshape(-1,1)\n",
    "\n",
    "# Weight & bias\n",
    "W = np.random.rand(1,1)\n",
    "b = np.random.rand(1)\n",
    "\n",
    "# loss function\n",
    "def loss_func(x,t):\n",
    "    \n",
    "    y = np.dot(x,W) + b\n",
    "    \n",
    "    return np.mean(np.power((t-y),2))\n",
    "\n",
    "# learning_rate\n",
    "learning_rate = 1e-4\n",
    "f = lambda x : loss_func(x_data,t_data)\n",
    "\n",
    "# 학습 진행\n",
    "for step in range(300000):\n",
    "    \n",
    "    W -= learning_rate * numerical_derivative(f,W)\n",
    "    b -= learning_rate * numerical_derivative(f,b)\n",
    "    \n",
    "    if step % 30000 == 0 :\n",
    "        print('W : {}, b : {}, loss : {}'.format(W,b,loss_func(x_data,t_data)))\n",
    "        \n",
    "# 예측\n",
    "def predict(x):\n",
    "    \n",
    "    return np.dot(x,W) + b\n",
    "\n",
    "# 시각화\n",
    "plt.scatter(x_data,t_data)\n",
    "plt.plot(x_data, np.dot(x_data,W)+b, color='r')\n",
    "plt.show()\n",
    "\n",
    "print(predict(62))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_data = np.array([62])\n",
    "scaled_predict_data = scaler_x.transform(predict_data.reshape(-1,1)) # 예측할 값도 정규화를 거쳐야 함\n",
    "print(scaled_predict_data)\n",
    "\n",
    "scaled_result = predict(scaled_predict_data)  # 예측결과(정규화)\n",
    "result = scaler_t.inverse_transform(scaled_result)\n",
    "print('예측된 Ozone량은 : {}'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow를 이용한 Linear Regression\n",
    "# Tensorflow는 버전이 1.x와 2.x 버전이 존재\n",
    "# 학습을 위해 우선은 1.15 버전 설치 \n",
    "# pip install tensorflow==1.15\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "node = tf.constant('Hello World')  # Node를 생성했어요! \n",
    "\n",
    "# 우리가 만든 Graph를 실행하기 위해서 Session이 필요!\n",
    "sess = tf.Session()\n",
    "\n",
    "# runner인 session이 생성되었으니 이걸 이용해서 node를 실행해 보아요!\n",
    "print(sess.run(node).decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# node를 2개 만들어요!\n",
    "node1 = tf.constant(10, dtype=tf.float32)\n",
    "node2 = tf.constant(20, dtype=tf.float32)\n",
    "\n",
    "node3 = node1 + node2\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "print(sess.run([node3,node2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node = tf.constant('Hello World')  # Node 생성\n",
    "\n",
    "# 우리가 만든 Graph를 실행하기 위해서 Session이 필요\n",
    "sess = tf.Session()\n",
    "\n",
    "# runner인 session이 생성되었으니 이걸 이용해서 node를 실행\n",
    "print(sess.run(node).decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholder를 이용\n",
    "# 2개의 수를 입력으로 받아서 덧셈연산을 수행\n",
    "import tensorflow as tf\n",
    "\n",
    "node1 = tf.placeholder(dtype=tf.float32)  # scalar 형태의 값 1개를 실수로 받아들일 수\n",
    "                                          # 있는 placeholder\n",
    "    \n",
    "node2 = tf.placeholder(dtype=tf.float32)  \n",
    "\n",
    "node3 = node1 + node2\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "print(sess.run(node3, feed_dict={ node1 : 20, node2 : 40}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 1. Raw Data Loading\n",
    "# 2. Data Preprocessing(데이터전처리)\n",
    "\n",
    "# 3. Training Data Set\n",
    "x_data = [2,4,5,7,10]\n",
    "t_data = [7,11,13,17,23]\n",
    "\n",
    "# 4. Weight & bias\n",
    "W = tf.Variable(tf.random.normal([1]), name='weight')   # W = np.random.rand(1,1)\n",
    "b = tf.Variable(tf.random.normal([1]), name='bias')     # b = np.random.rand(1) \n",
    "\n",
    "# 5. Hypothesis, Simple Linear Regression Model\n",
    "H = W * x_data + b\n",
    "\n",
    "# 6. Loss function\n",
    "loss = tf.reduce_mean(tf.square(t_data-H))\n",
    "\n",
    "# 7. train node 생성\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# 8. 실행준비 및 초기화작업\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())  # 초기화 작업\n",
    "\n",
    "# 9. 반복해서 학습을 진행!!\n",
    "for step in range(30000):\n",
    "    _, W_val, b_val, loss_val = sess.run([train,W,b,loss])\n",
    "    \n",
    "    if step % 3000 == 0:\n",
    "        print('W : {}, b : {}, loss : {}'.format(W_val, b_val, loss_val))\n",
    "        \n",
    "print(sess.run(H))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow를 이용해 Simple Linear Regression 학습하고\n",
    "# 예측하는 코드 작성\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# 1. Raw Data Loading\n",
    "\n",
    "# 2. Data Preprocessing\n",
    "# 3. Traning Data Set\n",
    "x_data = [1,2,3,4,5]\n",
    "t_data = [2,4,6,8,10]\n",
    "\n",
    "# 4. Placeholder\n",
    "X = tf.placeholder(dtype=tf.float32)\n",
    "T = tf.placeholder(dtype=tf.float32)\n",
    "\n",
    "# 5. Weight & bias\n",
    "W = tf.Variable(tf.random.normal([1]), name='weight')# Tensorflow에서는 shape을 표현할 때 []로 표현\n",
    "b = tf.Variable(tf.random.normal([1]), name='bias')\n",
    "\n",
    "# 6. Simple Linear Regression Model(Hypothesis)\n",
    "H = W * X + b\n",
    "\n",
    "# 7. Loss Function\n",
    "loss = tf.reduce_mean(tf.square(H-T))\n",
    "\n",
    "# 8. train 노드 생성\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss)\n",
    "\n",
    "# 9. Session & 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 10. 학습 진행(Graph 실행)\n",
    "for step in range(30000) :\n",
    "    _, W_val, b_val, loss_val = sess.run([train,W,b,loss], feed_dict={X :x_data, T : t_data})\n",
    "    \n",
    "    if step % 3000 == 0 :\n",
    "        print('W : {}, b : {}, loss : {}'.format(W_val,b_val,loss_val))\n",
    "\n",
    "# 11. Predict\n",
    "print(sess.run(H, feed_dict={X : [6]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 온도, 태양광, 바람에 따른 오존량을 학습해서 에측하는 Multiple Linear Regression\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Raw Data Loading\n",
    "df = pd.read_csv('./data/ozone.csv')\n",
    "\n",
    "# Data Preprocessing\n",
    "# 결측치 제거\n",
    "df = df.dropna(how='any')\n",
    "df = df[['Ozone','Solar.R','Wind','Temp']]\n",
    "\n",
    "# 이상치 제거\n",
    "zscore_threshold = 2  # zscore outlier 임계값 (일반적으로 2)\n",
    "\n",
    "for col in df.columns:\n",
    "    outliers = df[col][np.abs(stats.zscore(df[col])) > zscore_threshold]\n",
    "    df = df.loc[~df[col].isin(outliers)]\n",
    "\n",
    "# 정규화 처리\n",
    "scaler_x = MinMaxScaler()  # MinMaxScaler 클래스 객체 생성\n",
    "scaler_t = MinMaxScaler()\n",
    "\n",
    "training_data_x = df.iloc[:,1:]\n",
    "training_data_t = df['Ozone'].values.reshape(-1,1)\n",
    "scaler_x.fit(training_data_x)\n",
    "scaler_t.fit(training_data_t)\n",
    "\n",
    "training_data_x = scaler_x.transform(training_data_x)\n",
    "training_data_t = scaler_t.transform(training_data_t)\n",
    "\n",
    "# Training Data Set\n",
    "x_data = training_data_x\n",
    "t_data = training_data_t\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None,3], dtype=tf.float32)\n",
    "T = tf.placeholder(shape=[None,1], dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "W = tf.Variable(tf.random.normal([3,1]), name='weight')\n",
    "b = tf.Variable(tf.random.normal([1]), name='bias')\n",
    "\n",
    "# Multiple Linear Regression Model(Hypothesis)\n",
    "H = tf.matmul(X,W) + b\n",
    "\n",
    "# Loss Function\n",
    "loss = tf.reduce_mean(tf.square(H-T))\n",
    "\n",
    "# train 노드 생성\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss)\n",
    "\n",
    "# Session & 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 1학습 진행(Graph 실행)\n",
    "for step in range(30000) :\n",
    "    _, W_val, b_val, loss_val = sess.run([train,W,b,loss], feed_dict={X : x_data, T : t_data})\n",
    "    \n",
    "    if step % 3000 == 0 :\n",
    "        print('W : {}, b : {}, loss : {}'.format(W_val,b_val,loss_val))\n",
    "\n",
    "# Predict\n",
    "# 예측에 이용할 독립변수 역시 정규화 시켜준 후에 이용\n",
    "# 예측 결과 역시 inverse 정규화하여 값으로 표현\n",
    "predict_data_x = np.array([[150,8.0,85]])\n",
    "predict_data_x = scaler_x.transform(predict_data_x)\n",
    "result = sess.run(H, feed_dict={X : predict_data_x})\n",
    "result = scaler_t.inverse_transform(result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification(분류)\n",
    "# Training Data Set의 특성과 분포를 파악한 후\n",
    "# 미지의 입력데이터에 대해 어떤 종류의 값으로 분류될 수 있는지 예측\n",
    "# Classification의 장점 => 정확도 측정이 가능\n",
    "\n",
    "# Logistic Regression\n",
    "# 정확도가 상당히 높은 모델\n",
    "# Deep Learning의 기본 component\n",
    "# Linear Regression을 이용해서 Train Data Set의 특성과 분포를 파악\n",
    "# => 직선을 찾음(2차원 대상)\n",
    "# 그 직선을 기준으로 데이터를 분류(Classification)\n",
    "# 0 or 1로 출력(확률)\n",
    "# Linear Regression + Classification의 과정을 Logistic Regression이라 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "import mglearn  # Dataset을 가져오기 위한 utility module\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x,y = mglearn.datasets.make_forge()\n",
    "# x 데이터로 분포를 표시하고, y 데이터로 점인지 세모인지를 분류\n",
    "mglearn.discrete_scatter(x[:,0], x[:,1], y)\n",
    "\n",
    "# Linear Regression으로 x의 데이터를 학습\n",
    "# 각 데이터를 가장 잘 표현할 수 있는 직선을 그림\n",
    "model = linear_model.LinearRegression()\n",
    "model.fit(x[:,0].reshape(-1,1), x[:,1].reshape(-1,1))\n",
    "plt.plot(x[:,0], x[:,0] * model.coef_.ravel() + model.intercept_)  # model.coef_가 matrix형태여서 ravle()로 array 형태로 바꿔줌\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression만으로 분류가 되지 않는 것을 보이는 예제\n",
    "# 공부시간에 따른 시험합격여부에 대한 데이터를 학습하고\n",
    "# 특정 공부시간을 입력했을 때의 합격여부\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "\n",
    "# Training Data Set\n",
    "x_data = np.array([1,2,5,8,10])  # 공부시간\n",
    "t_data = np.array([0,0,0,1,1])   # 시헙합격 여부\n",
    "\n",
    "# Linear Regression model 생성, 학습, 그래프 출력\n",
    "model = linear_model.LinearRegression()\n",
    "model.fit(x_data.reshape(-1,1), t_data.reshape(-1,1))\n",
    "\n",
    "fig = plt.figure()\n",
    "fig_1 = fig.add_subplot(1,2,1)\n",
    "fig_1.scatter(x_data, t_data)\n",
    "fig_1.plot(x_data, x_data * model.coef_.ravel() + model.intercept_, color = 'r')\n",
    "# 이렇게 결과를 내게 되면 Linear Regression으로도 충분히 분류가 가능해 보임\n",
    "# 그러나 데이터 변동이 생기면 분류에 큰 리스크가 생김\n",
    "\n",
    "x_data = np.array([1,2,5,8,10,30])  # 공부시간\n",
    "t_data = np.array([0,0,0,1,1,1])   # 시헙합격 여부\n",
    "\n",
    "# Linear Regression model 생성, 학습, 그래프 출력\n",
    "model = linear_model.LinearRegression()\n",
    "model.fit(x_data.reshape(-1,1), t_data.reshape(-1,1))\n",
    "\n",
    "fig_2 = fig.add_subplot(1,2,2)\n",
    "fig_2.scatter(x_data, t_data)\n",
    "fig_2.plot(x_data, x_data * model.coef_.ravel() + model.intercept_, color = 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이러한 문제를 해결하기 위해 함수를 도입 => Sigmoid\n",
    "# 'S'자 모양의 곡선으로 변환 => 최소 0, 최대 1\n",
    "\n",
    "# Sigmoid 모양 확인\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_data = np.arange(-7,8)\n",
    "y_data = 1 / (1 + np.exp(-1 * x_data))\n",
    "\n",
    "plt.plot(x_data,y_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W : [[-0.02997247]\n",
      " [-0.11735252]], b : [-0.03647658], loss : 0.7948572039604187\n",
      "W : [[0.15168911]\n",
      " [0.44600835]], b : [-0.32973897], loss : 0.4941769242286682\n",
      "W : [[0.17593114]\n",
      " [0.66357607]], b : [-0.6693673], loss : 0.43918800354003906\n",
      "W : [[0.20768212]\n",
      " [0.7943966 ]], b : [-0.98014635], loss : 0.4008474349975586\n",
      "W : [[0.23875406]\n",
      " [0.8893843 ]], b : [-1.2588503], loss : 0.37158116698265076\n",
      "W : [[0.26745692]\n",
      " [0.9658872 ]], b : [-1.5087988], loss : 0.3485044538974762\n",
      "W : [[0.29359892]\n",
      " [1.0312313 ]], b : [-1.7343098], loss : 0.3298843502998352\n",
      "W : [[0.3173953]\n",
      " [1.0890026]], b : [-1.9393535], loss : 0.3145569860935211\n",
      "W : [[0.33914956]\n",
      " [1.1411879 ]], b : [-2.1272657], loss : 0.30171284079551697\n",
      "W : [[0.35915568]\n",
      " [1.1890059 ]], b : [-2.3007727], loss : 0.29077669978141785\n",
      "[[0.81982785]]\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression에서는 Sigmoid 함수를 도입하면서\n",
    "# local minima가 생기게 되고, 최소제곱법을 바로 사용 못하게 됨\n",
    "# 따라서 Loss Function 역시 바뀜 => Cross Entropy\n",
    "# Cross Entropy를 사용하면 E(w,b) 그래프가 다시 볼록함수 형태로 변환\n",
    "# 그러므로, gradient descent를 사용 가능\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Training Data Set\n",
    "x_data = np.array([[1,0],\n",
    "                   [2,0],\n",
    "                   [5,1],\n",
    "                   [2,3],\n",
    "                   [3,3],\n",
    "                   [8,1],\n",
    "                   [10,0]])\n",
    "t_data = np.array([[0],\n",
    "                   [0],\n",
    "                   [0],\n",
    "                   [1],\n",
    "                   [1],\n",
    "                   [1],\n",
    "                   [1]])\n",
    "\n",
    "# Placeholder\n",
    "X = tf.placeholder(shape=[None,2], dtype=tf.float32)\n",
    "T = tf.placeholder(shape=[None,1], dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "W = tf.Variable(tf.random.normal([2,1]), name='weight')\n",
    "b = tf.Variable(tf.random.normal([1]), name='bias')\n",
    "\n",
    "# Logistic Regression Model(Hypothesis)\n",
    "logits = tf.matmul(X,W) + b  # Linear Regression Hypothesis\n",
    "H = tf.sigmoid(logits)\n",
    "\n",
    "# Loss function\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=T))\n",
    "\n",
    "# Train node 생성\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss)\n",
    "\n",
    "# Session & 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습 진행(Graph 실행)\n",
    "for step in range(30000) :\n",
    "    _, W_val, b_val, loss_val = sess.run([train,W,b,loss], feed_dict={X : x_data, T : t_data})\n",
    "    \n",
    "    if step % 3000 == 0 :\n",
    "        print('W : {}, b : {}, loss : {}'.format(W_val,b_val,loss_val))\n",
    "        \n",
    "# Predict\n",
    "print(sess.run(H, feed_dict={X : [[4,2]]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data_env] *",
   "language": "python",
   "name": "conda-env-data_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
