{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python 결과값\n",
      "예측결과 : 1, 확률 : 0.5798057103407672\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression을 python, sklearn, tensorflow로 각각 구현\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import linear_model\n",
    "\n",
    "# 수치 미분함수\n",
    "from my_library.machine_learning_library import numerical_derivative\n",
    "\n",
    "# Raw Data Loading + Data Preprocessing\n",
    "\n",
    "# Training Data Set\n",
    "x_data = np.arange(2,21,2).reshape(-1,1)\n",
    "t_data = np.array([0,0,0,0,0,0,1,1,1,1]).reshape(-1,1)\n",
    "\n",
    "# 1. python 구현\n",
    "\n",
    "# Weight & bias\n",
    "W = np.random.rand(1,1)\n",
    "b = np.random.rand(1)\n",
    "\n",
    "# Loss Function\n",
    "def loss_func(input_obj):\n",
    "    # input_obj : W와 b를 같이 포함하고 있는 ndarray => [W1,W2,W3, ... , b]\n",
    "    num_of_bias = b.shape[0]\n",
    "    input_W = input_obj[:-1 * num_of_bias].reshape(-1,num_of_bias)  # 행렬연산을 하기 위한 W 추출\n",
    "    input_b = input_obj[-1 * num_of_bias]\n",
    "    \n",
    "    # 우리 모델의 예측값 : Linear Regression model(Wx + b) ==> sigmoid 적용\n",
    "    z = np.dot(x_data,input_W) + input_b\n",
    "    y = 1 / (1 + np.exp(-1*z))\n",
    "    \n",
    "    delta = 1e-7 #  0에 가까운 작은 값을 줌으로써 프로그램의 로그 연산시 무한대로 발산하는 것을 방지\n",
    "    \n",
    "    # Cross Entropy\n",
    "    return -np.sum(t_data * np.log(y+delta) + ((1-t_data) * np.log(1-y+delta)))\n",
    "    \n",
    "# Learning rate\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# 학습\n",
    "for step in range(30000):\n",
    "    \n",
    "    input_param = np.concatenate((W.ravel(), b.ravel()), axis = 0)\n",
    "    derivative_result = learning_rate * numerical_derivative(loss_func, input_param)\n",
    "    \n",
    "    \n",
    "    num_of_bias = b.shape[0]\n",
    "    \n",
    "    W = W - derivative_result[:-1 * num_of_bias].reshape(-1,num_of_bias)\n",
    "    b = b - derivative_result[-1 * num_of_bias]\n",
    "    \n",
    "    \n",
    "# Prediction => W,b를 구해서 Logistic Regression Model을 완성\n",
    "def logistic_predict(x):\n",
    "    \n",
    "    z = np.dot(x,W) + b\n",
    "    y = 1 / (1 + np.exp(-1 * z))\n",
    "    \n",
    "    if y < 0.5 :\n",
    "        result = 0\n",
    "    else :\n",
    "        result = 1\n",
    "    \n",
    "    return result, y\n",
    "\n",
    "input_value = np.array([[13]])\n",
    "result = logistic_predict(input_value)\n",
    "print('python 결과값')\n",
    "print('예측결과 : {}, 확률 : {}'.format(result[0],result[1][0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python 결과값\n",
      "예측결과 : [0], 확률 : [[0.50009391 0.49990609]]\n"
     ]
    }
   ],
   "source": [
    "# 2. sklearn 구현\n",
    "\n",
    "# Logistic Regression Model 생성\n",
    "model = linear_model.LogisticRegression()\n",
    "\n",
    "# Traning Data Set을 이용한 학습\n",
    "model.fit(x_data,t_data.ravel())\n",
    "\n",
    "# Prediction\n",
    "input_value = np.array([[13]])\n",
    "pred_val = model.predict(input_value)\n",
    "pred_proba = model.predict_proba(input_value)\n",
    "print('sklearn 결과값')\n",
    "print('예측결과 : {}, 확률 : {}'.format(pred_val,pred_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow 결과값\n",
      "[[0.58202696]]\n"
     ]
    }
   ],
   "source": [
    "# 3. tensorflow 구현\n",
    "\n",
    "# Placeholder 생성\n",
    "X = tf.placeholder(dtype=tf.float32)  # 독립변수가 1개인 경우(simple), shape 명시 x\n",
    "T = tf.placeholder(dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "W = tf.Variable(tf.random.normal([1,1]), name='weight')\n",
    "b = tf.Variable(tf.random.normal([1]), name='weight')\n",
    "\n",
    "# Hypothesis\n",
    "logit = W * X + b\n",
    "H = tf.sigmoid(logit)\n",
    "\n",
    "# Loss Function\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit, labels=T))\n",
    "\n",
    "# Train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-3).minimize(loss)\n",
    "\n",
    "# Session & 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "for step in range(30000):\n",
    "    sess.run(train, feed_dict={X : x_data, T : t_data})\n",
    "    \n",
    "# Prediction\n",
    "input_value = np.array([13])\n",
    "result = sess.run(H, feed_dict={X : input_value})\n",
    "print('tensorflow 결과값')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data_env] *",
   "language": "python",
   "name": "conda-env-data_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
