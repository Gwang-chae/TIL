# Deep Learning Interview

* 출처

  https://www.analyticsvidhya.com/blog/2020/04/comprehensive-popular-deep-learning-interview-questions-answers/

* 해당 글을 일부 번역, 개략적으로 추가 정리하였습니다.

<br>

## 1. 퍼셉트론과 로지스틱 회귀의 차이점은?

* 다층 퍼셉트론과 로지스틱 회귀분석은 이진 분류 문제에 사용된다는 공통점이 존재
* 그러나 다층 퍼셉트론의 경우 출력이 0 또는 1로 고정되는 반면, 로지스틱 회귀의 경우 0과 1사이의 확률을 출력



## 3. 신경망에 활성화 함수를 사용하지 않으면 어떻게 되는가?(활성화 함수의 역할)

* 일반적으로 노드에 들어오는 입력은 활성화 함수를 거쳐 다음 레이어로 전달되며, 이 때 사용되는 함수는 비선형 함수
* 활성화 함수를 사용하지 않게 되면 데이터의 복잡한 패턴을 알아낼 수 없음
* 예를 들어 Z = w*x 의 형태를 가진 3-layer 구조를 가정해보면
  * Y = w^3 * x의 값이 나올 것임 -> 이것은 하나의 layer로 표현 가능
* 결과적으로 linear한 연산의 layer들은 몇개를 쌓더라도, 하나의 linear 연산에 불과
* 그러므로, 비선형 함수를 활성화 함수로 사용하지 않으면 이것은 결국 선형 회귀 모델에 불과



## 4. 신경망에서 모든 가중치가 같은 값으로 초기화되면 어떻게 되는가?

* 만약 모든 뉴런이 같은 값의 가중치를 갖는다면, 각 hidden unit들은 같은 입력값들을 받게 됨
* 순전파 시에는 문제가 되지 않지만, 역전파를 하게 될 때 손실 함수의 미분값들은 항상 같게 됨
* 따라서, 네트워크의 학습이 이루어지지 않게 되며 데이터의 어떤 패턴도 학습하지 못하게 됨
* Underfitting 발생



## 6. 신경망에서 가중치와 편향의 역할은?

* `가중치` : 입력신호가 결과 출력에 주는 영향도를 조절하는 매개변수
* `편향` : 뉴런의 활성화 조건을 조절하는 매개변수
  * ex) bias가 -0.1일 경우, 각 입력 신호의 가중치 곱들의 합이 0.1을 넘어야 뉴런이 활성화



## 7. 딥러닝에서 순전파와 역전파는 어떻게 작용하는가?

* `순전파`  
  * input이 은닉층을 거쳐 가중치와 연산을 수행하고 활성화 함수를 통해 결과값을 다음 layer에 전달함. 이런 과정을 출력층까지 수행함
  * 이처럼 순차적으로 입력부터 출력까지 신호를 전파하며 값을 갱신해 나가는 것을 순전파
* `역전파`
  * 출력된 최종 결과값(예측값)과 실제값의 차이인 오차를 계산하여 순전파와는 반대 방향으로 가중치를 수정
  * 하나의 출력값인 오차는 이전 layer의 여러 노드의 영향으로 만들어짐
  * 이 이전 layer의 노드들의 가중치가 오차에 미친 영향을 조정하여 오차를 감소시키는 것이 역전파



## 8. 딥러닝에 사용되는 일반적인 데이터 구조는?

* 딥러닝은 list와 같이 단순한 데이터 구조부터 computation graph처럼 복잡한 데이터 구조까지도 사용

* `List` : 원소들의 시퀀스
* `Matrix` : 행과 열로 이루어진 원소들의 시퀀스
* `Dataframe` : `Matrix`와 유사하지만, 데이터셋의 각 데이터 지점을 나타내는 열 이름과 행이 포함된 실제 데이터를 가짐
  * ex) `Dataframe`에 100명의 학생 정보를 저장하면 각 학생들의 세부정보가 열에 따른 구조로 정렬, 각 행은 각각의 학생 데이터를 나타냄
* `Tensor` : PyTorch와 TensorFlow에 사용되며, 딥러닝의 기본적인 프로그래밍 유닛임. 다차원 배열처럼 많은 양의 수학적 연산을 수행할 수 있음.
* `Computation Graph` : 딥러닝은 다량의 layer들과 parameter들을 포함함. `Computation Graph`는 각 노드가 신경망에서 수행하는 연산 또는 구성요소를 나타내 줌. 수학식을 표현하고 평가하는 하나의 방법



## 9. 우리는 왜 Batch Normalization을 사용해야 하는가?

* `Batch Noramlization`은 딥러닝의 학습시간을 단축하는 하나의 기술
* 정규화된 입력값이 로지스틱 회귀분석 모델의 성능 개선에 도움이 되는 것처럼,  딥러닝에서도 은닉층을 지난 값들을 정규화할 수 있음

* 입력 데이터가 입력층을 지나면 정규화되기란 쉽지 않음. 그러나 `Batch Normalization`을 통해 이 값들을 정규화함으로써 학습을 더 빠르게 하고 `Local Minimum`에 빠질 가능성을 줄일 수 있음



## 10. 활성화 함수를 나열해 보세요.

* `Sigmoid` : 0과 1사이의 출력값을 가짐
  * 특징 
    * 1. Saturation : 입력 신호의 총합이 크거나 작을 때 기울기가 0에 가까워지는 현상, `vanishing gradient` 문제 야기
      2. non zero-centered : 가중치는 항상 같은 방향으로 이동. zig-zag 형태로 가중치가 업데이트 되지 못하면서 학습 효율 감소
* `Tanh` : -1과 1사이의 출력값을 가짐
  * 특징
    * 	1. Saturation
       	2. zero-centered : non zero-centered한 `Sigmoid`의 단점 보완
* `Relu`: max(0, x)의 출력값을 가짐
  * 특징
    * 	1. x가 양수인 범위에서는 Saturate하지 않음 -> 음수인 범위에서는 Saturate
       	2. 계산 효율이 좋음
       	3. non zero-centered
* `Leaky Relu`: max(0.01*x, x)의 출력값을 가짐
  * 특징
    * 1. 양수인 범위에서만 Saturate하지 않던 `Relu`의 단점 보완 -> 음수인 범위에서도 Saturate하지 않음
      2. 계산 효율이 좋음
      3. Dead Relu 현상 예방
* `Softmax` : 0과 1사이의 출력값을 가짐
  * 특징
    * `Sigmoid`와 같이 출력층에서 주로 사용
    * 다중 분류에 사용
    * 확률의 총합이 1이므로 각 분류에 속할 확률을 비교 가능